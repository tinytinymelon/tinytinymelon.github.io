{"meta":{"title":"梅子黄时雨","subtitle":"","description":"","author":"Leo Zhou","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"UTLK: Chapter 3 (1)","slug":"UTLK-Chapter-3-1","date":"2020-01-17T14:05:26.000Z","updated":"2020-01-17T14:06:44.016Z","comments":true,"path":"2020/01/17/UTLK-Chapter-3-1/","link":"","permalink":"http://yoursite.com/2020/01/17/UTLK-Chapter-3-1/","excerpt":"","text":"Reference O(n)、O(1)和CFS调度器 There’s A New Linux CPU Scheduler Based Upon BFS 1. task_structstruct task_struct定义在include/linux/sched.h中： 123456789struct task_struct &#123;#ifdef CONFIG_THREAD_INFO_IN_TASK /* * For reasons of header soup (see current_thread_info()), this * must be the first element of task_struct. */ struct thread_info thread_info;#endif... 这里这么说的缘故是，current_thread_info的通用定义如下： 123456789#ifdef CONFIG_THREAD_INFO_IN_TASK/* * For CONFIG_THREAD_INFO_IN_TASK kernels we need &lt;asm/current.h&gt; for the * definition of current, but for !CONFIG_THREAD_INFO_IN_TASK kernels, * including &lt;asm/current.h&gt; can cause a circular dependency on some platforms. */#include &lt;asm/current.h&gt;#define current_thread_info() ((struct thread_info *)current)#endif 直接通过真专的转换来完成的，因此需要将thread_info放置在struct头部。 1.1. struct thread_info每个arch都有不同的定义。 x86: 1234struct thread_info &#123; unsigned long flags; /* low level flags */ u32 status; /* thread synchronous flags */&#125;; ARM： 123456789101112131415161718192021222324252627/* * low level task data that entry.S needs immediate access to. * __switch_to() assumes cpu_context follows immediately after cpu_domain. */struct thread_info &#123; unsigned long flags; /* low level flags */ int preempt_count; /* 0 =&gt; preemptable, &lt;0 =&gt; bug */ mm_segment_t addr_limit; /* address limit */ struct task_struct *task; /* main task structure */ __u32 cpu; /* cpu */ __u32 cpu_domain; /* cpu domain */#ifdef CONFIG_STACKPROTECTOR_PER_TASK unsigned long stack_canary;#endif struct cpu_context_save cpu_context; /* cpu context */ __u32 syscall; /* syscall number */ __u8 used_cp[16]; /* thread used copro */ unsigned long tp_value[2]; /* TLS registers */#ifdef CONFIG_CRUNCH struct crunch_state crunchstate;#endif union fp_state fpstate __attribute__((aligned(8))); union vfp_state vfpstate;#ifdef CONFIG_ARM_THUMBEE unsigned long thumbee_state; /* ThumbEE Handler Base register */#endif&#125;; ARM64 12345678910111213141516171819202122/* * low level task data that entry.S needs immediate access to. */struct thread_info &#123; unsigned long flags; /* low level flags */ mm_segment_t addr_limit; /* address limit */#ifdef CONFIG_ARM64_SW_TTBR0_PAN u64 ttbr0; /* saved TTBR0_EL1 */#endif union &#123; u64 preempt_count; /* 0 =&gt; preemptible, &lt;0 =&gt; bug */ struct &#123;#ifdef CONFIG_CPU_BIG_ENDIAN u32 need_resched; u32 count;#else u32 count; u32 need_resched;#endif &#125; preempt; &#125;;&#125;; RISC-V 123456789101112131415161718192021/* * low level task data that entry.S needs immediate access to * - this struct should fit entirely inside of one cache line * - if the members of this struct changes, the assembly constants * in asm-offsets.c must be updated accordingly * - thread_info is included in task_struct at an offset of 0. This means that * tp points to both thread_info and task_struct. */struct thread_info &#123; unsigned long flags; /* low level flags */ int preempt_count; /* 0=&gt;preemptible, &lt;0=&gt;BUG */ mm_segment_t addr_limit; /* * These stack pointers are overwritten on every system call or * exception. SP is also saved to the stack it can be recovered when * overwritten. */ long kernel_sp; /* Kernel stack pointer */ long user_sp; /* User stack pointer */ int cpu;&#125;; x86竟然出奇的简单，剩下的arch都带有各种其他信息，不确定这里的信息只是作为display还是会参与到进程调度中。 1.2. process statestate定义很简单，是一个long： 12/* -1 unrunnable, 0 runnable, &gt;0 stopped: */volatile long state; 这里的state可以作为一个flag的组合存在，因此state分成几类。 2.6.19 kernel: 1234567#define TASK_RUNNING 0#define TASK_INTERRUPTIBLE 1#define TASK_UNINTERRUPTIBLE 2#define TASK_STOPPED 4#define TASK_TRACED 8#define EXIT_ZOMBIE 16#define EXIT_DEAD 32 以上是比较通用的几个，在后续的版本中加入了更多的状态，比如 5.4.rc kernel（在3.14 kernel以后加入）: 123456789101112131415161718/* Used in tsk-&gt;state: */#define TASK_RUNNING 0x0000#define TASK_INTERRUPTIBLE 0x0001#define TASK_UNINTERRUPTIBLE 0x0002#define __TASK_STOPPED 0x0004#define __TASK_TRACED 0x0008/* Used in tsk-&gt;exit_state: */#define EXIT_DEAD 0x0010#define EXIT_ZOMBIE 0x0020#define EXIT_TRACE (EXIT_ZOMBIE | EXIT_DEAD)/* Used in tsk-&gt;state again: */#define TASK_PARKED 0x0040#define TASK_DEAD 0x0080#define TASK_WAKEKILL 0x0100#define TASK_WAKING 0x0200#define TASK_NOLOAD 0x0400#define TASK_NEW 0x0800#define TASK_STATE_MAX 0x1000 1.2.1. TASK_PARKED主要是为了cpu hotplug引入的，以后再详细分析。 1.2. Thread stack每一个user thread都有两个stack：user space stack和kernel space stack。 stack的为一个thread_union，注意它是从高地址往低地址生长的： 123456789union thread_union &#123;#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK struct task_struct task;#endif#ifndef CONFIG_THREAD_INFO_IN_TASK struct thread_info thread_info;#endif unsigned long stack[THREAD_SIZE/sizeof(long)];&#125;; 这里的THREAD_SIZE是arch spcified： x86(32bit): PAGE_SIZE &lt;&lt; 1 = 2 * PAGE_SIZE x64(64bit): PAGE_SIZE &lt;&lt; 2 = 4 * PAGE_SIZE ARM &amp; ARM64: 2 * PAGE_SIZE RISC-V: 2 * PAGE_SIZE x64比较特殊，stack size是4个page size。具体两个stack是如何，在何时分配，如果切换后面在讨论。 1.3. Process Scheduler历史上有过很多的Process Scheduler的实现，有些被接收到了linux kernel官方code中，有些被引入linux kernel的衍生版本： global runqueue: O(n)，使用两个queue，一个保存待运行，一个保存消耗完时间片，主要问题在于插入process到有序的queue中很耗时间 O(1) scheduler:2.6.0提出，在2.6.23后被CFS替换掉。 使用两组(current/expired)140个priority array来保存各种不同的优先级的process 使用两个bitmap来分别表明current/expired中每个array是否还有process 找下一个运行的process就变成了搜索bit map的最地位的1所在位置，对应到相应的priority array current bitmap consumed以后可以交换current/expired array/bitmap的位置 RSDL(Rotating Staircase Deadline Scheduler): Con Kolivas提出，单和CFS竞争失败，没有merge到官方linux kernel，一度愤而离开linux kernel开发 也是2组140个priority array，但是每个process固定运行6ms，结束后放置到低一级array中(比如120 -&gt; 121)；最后从139 array出来后，放入expired array组相应的priority的array CFS(Completely Fair Schedulder): 2.6.23同时还引入了模块化的scheduler设计 模块分成两层 core scheduler layer -&gt; 负责负载均衡的讲task分配到每一个cpu core的runqueue上 specific scheduler layer -&gt; 自定义的针对不同场景，对于单个core的scheduler CFS使用红黑树来记录某个process的运行优先级，每个node代表一个process 红黑树的key用vruntime，vruntime(virual runtime)的增加取决于process优先级，级别越高，增长越慢 process运行都会增加vruntime，结束时候然后插回红黑树 每次从红黑树取最小vruntime的process运行 Brain Fuck Scheduler (BFS) schulder：又是Con Kolivas提出，依然没有被merge到官方linux kernel source code。 可以从http://ck.kolivas.org/拿到对应实现的patch，apply到你的linux kernel中 BFS使用单一链表保存所有process，记录vruntime和last run CPU 每一次schedule需要对整个列表进行扫描，因此是O(n) 不过在桌面级别使用，process数量不多，也就无所谓了 1.4. PID quick search为了加快各种pid的索引，几种hashtable被引入，加快索引，这些pid的索引包括： 12345678enum pid_type&#123; PIDTYPE_PID, // pid of process PIDTYPE_TGID, // pid of thread group leader process PIDTYPE_PGID, // pid of the group leader process PIDTYPE_SID, // pid of the session leader proess PIDTYPE_MAX&#125;; 在引入idr之前，这个hashtable定义在kernel/pid.c中，使用pidhash_init函数进行初始化： 1static struct hlist_head *pid_hash[PIDTYPE_MAX]; 结合hashmap，每一个task_struct中都包含了一个pid的数组： 12/* PID/PID hash table linkage. */struct pid pids[PIDTYPE_MAX]; 这里的struct pid关联了两个链表： 12345678struct pid&#123; /* Try to keep pid_chain in the same cacheline as nr for find_pid */ int nr; struct hlist_node pid_chain; /* list of pids with the same nr, only one of them is in the hash */ struct list_head pid_list;&#125;; 所有的hash内容指向了这样一个pid，而pid中的pid_chain指向hashtable中得到相同hash值的其他pid；而pid_list指向其他符合这个条件的task_struct中的pid。 在4.15-rc1中，引入了idr来代替pid bitmap；同时，用于记录pid信息的hashtable也被移除，相关的逻辑放入到了pid struct中： 123456789struct pid&#123; atomic_t count; unsigned int level; /* lists of tasks that use this pid */ struct hlist_head tasks[PIDTYPE_MAX]; // 记录use这个pid作为tgid，pgid的所有process列表 struct rcu_head rcu; struct upid numbers[1];&#125;; 因此，此时寻/搜索找某个pid作为各种角色processes时候，就变成使用idr找到对应的pid struct，然后从pid-&gt;tasks中取到需要的process list。 tasks的初始化在pid被alloc时候进行： 123456struct pid *alloc_pid(struct pid_namespace *ns) &#123; ... for (type = 0; type &lt; PIDTYPE_MAX; ++type) INIT_HLIST_HEAD(&amp;pid-&gt;tasks[type]); ...&#125; 详细逻辑在PID分配中解释。 1.5. PID 分配在64bit情况下，linux可以分配的pid个数有400多万个(4 1024 1024);否则只有32768 (8 * 4096)个： 12345678910/* * This controls the default maximum pid allocated to a process */#define PID_MAX_DEFAULT (CONFIG_BASE_SMALL ? 0x1000 : 0x8000)/* * A maximum of 4 million PIDs should be enough for a while: */#define PID_MAX_LIMIT (CONFIG_BASE_SMALL ? PAGE_SIZE * 8 : \\ (sizeof(long) &gt; 4 ? 4 * 1024 * 1024 : PID_MAX_DEFAULT)) 2.6.13和最新的5.5版本都是如此。只是如何进行分配的逻辑有所改动。 1.5.1. pidmap分配：2.6.13使用bitmap来记录是否pid已经被分配了： 1234567891011121314151617/* * PID-map pages start out as NULL, they get allocated upon * first use and are never deallocated. This way a low pid_max * value does not cause lots of bitmaps to be allocated, but * the scheme scales to up to 4 million PIDs, runtime. */typedef struct pidmap &#123; atomic_t nr_free; void *page;&#125; pidmap_t;static pidmap_t pidmap_array[PIDMAP_ENTRIES] = &#123; [ 0 ... PIDMAP_ENTRIES-1 ] = &#123; ATOMIC_INIT(BITS_PER_PAGE), NULL &#125; &#125;; #define PIDMAP_ENTRIES ((PID_MAX_LIMIT + 8*PAGE_SIZE - 1)/PAGE_SIZE/8)#define BITS_PER_PAGE (PAGE_SIZE*8)#define BITS_PER_PAGE_MASK (BITS_PER_PAGE-1) pidmap_array的entry每一个负责8 PAGE_SIZE的pid，因为一个byte包含了8bit，因此我们只需要8 PAGE_SIZe / 8 = PAGE_SIZE个bytes，刚好一个PAGE，因此，对于每一个pidmap中的page就是一个page。 1.5.2. idr分配：4.11-rc kernel之前参考： idr - integer ID management linux内核IDR机制详解【转】 Reimplement IDR and IDA using the radix tree idr内部使用了radix tree数据结构，在实现上面也有过一次reimplement(4.11-rc1)。 在此之前idr的实现都在idr.h中，相关数据结构包括struct idr_layer和struct idr，具体实现可以参考linux内核IDR机制详解【转】。 在32bit时候，每级层表使用5bits，一共32个entry指向下一层layer；这么做的好处在于可以用一个32bit(long)来作为bitmap；而64bit中使用6bits每层表，也是一个long。不过实际上无论32bit和64bit，有效的位数只用了31bit。 ida负责真正的分配id，其中包含了一个idr和另外的一个bitmap。 1.5.3. idr分配：4.11-rc kernel改进参考： Reimplement IDR and IDA using the radix tree Implementing IDR in __alloc_fd() 4.11-rc以后进行了reimplement, 直接使用内部的一个radix tree： 1234struct idr &#123; struct radix_tree_root idr_rt; unsigned int idr_next;&#125;; 12345678910111213141516171819202122/* * @count is the count of every non-NULL element in the -&gt;slots array * whether that is an exceptional entry, a retry entry, a user pointer, * a sibling entry or a pointer to the next level of the tree. * @exceptional is the count of every element in -&gt;slots which is * either radix_tree_exceptional_entry() or is a sibling entry for an * exceptional entry. */struct radix_tree_node &#123; unsigned char shift; /* Bits remaining in each slot */ unsigned char offset; /* Slot offset in parent */ unsigned char count; /* Total entry count */ unsigned char exceptional; /* Exceptional entry count */ struct radix_tree_node *parent; /* Used when ascending tree */ struct radix_tree_root *root; /* The tree we belong to */ union &#123; struct list_head private_list; /* For tree user */ struct rcu_head rcu_head; /* Used when freeing node */ &#125;; void __rcu *slots[RADIX_TREE_MAP_SIZE]; unsigned long tags[RADIX_TREE_MAX_TAGS][RADIX_TREE_TAG_LONGS];&#125;; 基本实现还是类似的，只是单独拎了出来；另外再晚一些的版本，radix_tree_node被定义成xa_node，基本定义保持一致： 123/* Keep unconverted code working */#define radix_tree_root xarray#define radix_tree_node xa_node 1.6. Namespace为了支持多层namespace(pid namespace本身是层级结构，一个process在同一层至多属于一个namespace node)，因此引入了upid的概念，记录每一层信息。 具体参考Linux系统如何标识进程？。 详细分析后续再做。。 1.7. Wait QueueWait queue发生过一次rename来保证内部名字的一致性：sched/wait: Standardize internal naming of wait-queue heads 大体上没有逻辑上的改变，只是名字改了。 有意思的是，你可以创建n多个wait queue，linux kernel提供了相应的宏和函数来帮助你做这些事情。 首先wait的queue的描述： 123456789101112131415/* * A single wait-queue entry structure: */struct wait_queue_entry &#123; unsigned int flags; void *private; wait_queue_func_t func; struct list_head entry;&#125;;struct wait_queue_head &#123; spinlock_t lock; struct list_head head;&#125;;typedef struct wait_queue_head wait_queue_head_t; 然后通过宏初始化wait entry，调用函数加入wait queue： 12345678910111213#define __WAITQUEUE_INITIALIZER(name, tsk) &#123; \\ .private = tsk, \\ .func = default_wake_function, \\ .entry = &#123; NULL, NULL &#125; &#125;#define DECLARE_WAITQUEUE(name, tsk) \\ struct wait_queue_entry name = __WAITQUEUE_INITIALIZER(name, tsk)// 使用操作：DECLARE_WAITQUEUE(wait, current);set_current_state(TASK_INTERRUPTIBLE);add_wait_queue(&amp;ctx-&gt;ctx_zombieq, &amp;wait); 最后可以使用wake_up进行唤醒。 1.8. process resource limit能够想到的资源控制包括两个方面： process本身的控制，定义在process的描述符中 cgroup的资源控制，针对一个namespace而言的 前者定义在task_struct -&gt; signal (struct signal_struct) -&gt; rlimRLIM_NLIMITS 其中struct rlimit定义如下： 1234struct rlimit &#123; __kernel_ulong_t rlim_cur; __kernel_ulong_t rlim_max;&#125;; RLIM_NLIMITS包含了一系列的资源，包括： Field names Index Description RLIMIT_CPU 0 Process能运行的最多seconds，如果超过了这个时间，那么process会收到一个SIGXCPU signal，如果随后process还没terminated，那么会发送一个SIGKILL。 RLIMIT_FSIZE 1 Process能用的最多file size，尝试超过额度分配时候会发送SIGXFSZ RLIMIT_DATA 2 Process能用的最多的heap size RLIMIT_STACK 3 max stack size RLIMIT_CORE 4 max core dump size RLIMIT_RSS 5 max number of page frames can be owned by one processs RLIMIT_NPROC 6 max number of process one user can own RLIMIT_NOFILE 7 max open file descriptor RLIMIT_MEMLOCK 8 max size of nonswappable memory, in bytes RLIMIT_AS 9 max process address space, in bytes. Kernel use malloc to check. RLIMIT_LOCKS 10 max number of file locks RLIMIT_SIGPENDING 11 max number of pending signals for the process RLIMIT_MSGQUEUE 12 max number of posix message queue RLIMIT_NICE 13 max nice prio allowed to raise to RLIMIT_RTPRIO 14 max realtime priority RLIMIT_RTTIME 15 timeout for RT tasks in us RLIM_NLIMITS 16 count of kinds of limit 许多resource超过limit时候会发送一个signal： 123456789101112131415161718192021static bool check_rlimit(u64 time, u64 limit, int signo, bool rt, bool hard)&#123; if (time &lt; limit) return false; if (print_fatal_signals) &#123; pr_info(&quot;%s Watchdog Timeout (%s): %s[%d]\\n&quot;, rt ? &quot;RT&quot; : &quot;CPU&quot;, hard ? &quot;hard&quot; : &quot;soft&quot;, current-&gt;comm, task_pid_nr(current)); &#125; __group_send_sig_info(signo, SEND_SIG_PRIV, current); return true;&#125;// 比如对于cpu limit/* At the soft limit, send a SIGXCPU every second */if (check_rlimit(ptime, softns, SIGXCPU, false, false)) &#123; sig-&gt;rlim[RLIMIT_CPU].rlim_cur = soft + 1; softns += NSEC_PER_SEC;&#125; 相应的控制函数包括： getrlimit setrlimit cgroup级别的resource控制，记录在于对应的cgroup之中，比如对于pid的限制，定义在struct pids_cgroup: 12345678910111213141516struct pids_cgroup &#123; struct cgroup_subsys_state css; /* * Use 64-bit types so that we can safely represent &quot;max&quot; as * %PIDS_MAX = (%PID_MAX_LIMIT + 1). */ atomic64_t counter; int64_t limit; /* Handle for &quot;pids.events&quot; */ struct cgroup_file events_file; /* Number of times fork failed because limit was hit. */ atomic64_t events_limit;&#125;; 这里的PID_MAX_LIMIT就是400w的那个pid上限。更多cgroup的限制可以参考:Control Group v2","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"Kernel","slug":"Kernel","permalink":"http://yoursite.com/tags/Kernel/"},{"name":"Memory","slug":"Memory","permalink":"http://yoursite.com/tags/Memory/"}]},{"title":"Linux Thead Model and Impl","slug":"Linux-Thead-Model-and-Impl","date":"2020-01-12T15:00:24.000Z","updated":"2020-01-12T15:00:44.243Z","comments":true,"path":"2020/01/12/Linux-Thead-Model-and-Impl/","link":"","permalink":"http://yoursite.com/2020/01/12/Linux-Thead-Model-and-Impl/","excerpt":"","text":"Reference POSIX Threads and the Linux Kernel Native POSIX Thread Library (NPTL): CSE 506 Don Porter linux futex浅析 Linux Kernel 2.4 线程实现机制分析 Basics of Futexes A futex overview and update Searchable Linux Syscall Table for x86 and x86_64 Linux 内核的同步机制，第 1 部分 1. POSIX Threads Model1.1. Definitions Process: 主要关注在resource上面 an address space and a group of resources all dedicated to running that program. Thread: 主要是一个process包含多个threads the resources necessary to represent that single thread of execution. 1.2. POSIX Thread Model vs Linux Task Model 类似黑名单，POSIX中大部分resource是一个process中所有thread shared，除了： CPU registers User stack Blocked signal mask 而linux类似白名单，大部分resource都是thread独占的，除了： Address space Signal handlers Open files Working directory 在这种情况下，POSIX的thread模型和linux的thread模型会有互相不兼容的问题，最明显的一个就是关于PID，PPID，credentials (user ID, group ID, etc), and pending signal mask. 换句话说，POSIX中一个process的所有thread是共享一个pid的，但是在linux中，这些所有的thread都有自己独立的pid。 这又进一步的带来了另外的问题，POSIX关于某个pid的操作是对于整个process而言的。无论是signal, exit, suspend/resume或者shared memory(address)的detach，在POSIX中都是对于一个process中的所有thread的集体操作。 但是在linux中，这些操作都是对于单个thread的操作，和整体process关系不直接关联。 1.3. User Space &amp; Kernel Space Thread Mapping M:1, 问题在于只有一个kernel thread，一旦被阻塞就整体玩完 1:1, 最容易实现，性能有牺牲，特别是context switch M:N，多线程情况下性能能够达到最优，但是实现复杂 2. LinuxThreadsLinuxThreads模型使用1:1的模型，但是和POSIX lib的不兼容，因此一个能在POSIX规范中能够运行的binary，不一定能在LinuxThreads lib中正常运行。 LinuxThreads另外一个特点是有一个Manager Thread。 2.1. Managed Thread除了线程调度(1:1模型直接有kernel完成)，其他诸如线程取消、线程间的同步等工作，都是在核外(User Space)线程库中完成的，由Manager Thread负责。 Manager Thread使用pipe管道来和其他线程进行通讯。管理线程的栈和用户线程的栈是分离的，管理线程在进程堆中通过malloc()分配一个THREAD_MANAGER_STACK_SIZE字节的区域作为自己的运行栈。 管理线程容易成为瓶颈，这是这种结构的通病；同时，管理线程又负责用户线程的清理工作，因此，尽管管理线程已经屏蔽了大部分的信号，但一旦管理线程死亡，用户线程就不得不手工清理了，而且用户线程并不知道管理线程的状态，之后的线程创建等请求将无人处理。 2.2. therad id和process id每个LinuxThreads线程都同时具有线程id和进程id，其中进程id就是内核所维护的进程号，而线程id则由LinuxThreads分配和维护。值得注意的是每一个thread都有自己的tid(unique in process)和pid(unique in whole os) 因此LinuxThreads的clone函数，对于CLONE_PID没有符合POSIX要求。另外，线程的数量被限制在1024，另外还会收到全局process可用数目的限制，因为这个thread其实就是一个process。 2.3. signal &amp; 同步LinuxThread中的同步依赖于信号，当接受到一个kill signal之后，manager thread会将其转发给所有其他的thread。同时因为没有线程组的概念，因此其他signal的发送都是对于单一thread的操作。 LinuxThreads将使用SIGUSR1和SIGUSR2作为内部使用的restart和cancel信号，这样应用程序就不能使用这两个原本为用户保留的信号了。 2.4. 其他问题还有一些其他和POSIX线程有关的调用LinuxThreads，比如nice、setuid、setrlimit等，都是不兼容POSIX的，只针对一个thread有效果。 3. NPTL2.6版本以后，linux kernel引入了和POSIX兼容的NPTL(Native POSIX Thread Library)多线程库。NPTL也是1:1的user和kernel space thread mapping。 3.1. futex (Fast userspace mutex)NPTL同时引入了futex，提供了一种用户thread的同步方式，具体包括了如下的特点： 首先尝试在user space使用atomic的方法来快速加锁。在大部分时间，类似java的light lock，能够快速的加锁。 如果加锁失败，那么thread可以尝试： 在原地spinning，等待lock free 调用syscal futex来加锁，这时候需要陷入linux kernel；futex加锁依是针对address的。 另外futex可以用来semaphores和mutex。 更加详细的syscall的列表可以在/include/uapi/asm-generic/unistd.h中找到。 3.2. pid &amp; tgid首先需要注意的是，两个不同的操作的结果： fork : 产生一个新的process pthread_create : 产生一个新的thread 但实际上fork和pthread_create都会调用到do_fork，因此所有的process都会有pid，tgid和ppid。 另外需要注意的是，即使在NPTL情况下，每一个thread实际还是lwp(light weight process)。 pid: process(thread)的id，每一个thread/process都不一样，全局唯一 ppid: 创建process(thread)的parent process的pid tgid(thread group id): 所有在同一个线程组中共享的id，对于线程组leader而言有pid = tgid NPTL为了兼容POSIX的操作，getpid或者对于process的操作会返回tgid或者对于所有包含相同tgid的process的操作。因此，其实linux kernel内部会有两套对于process/thread操作：一套是对于tgid，一套是真实对于单个thread的。","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"kernel","slug":"kernel","permalink":"http://yoursite.com/tags/kernel/"}]},{"title":"UTLK Chapter 2 读书笔记","slug":"UTLK-Chapter-2-读书笔记","date":"2020-01-05T07:54:45.000Z","updated":"2020-01-05T07:55:47.348Z","comments":true,"path":"2020/01/05/UTLK-Chapter-2-读书笔记/","link":"","permalink":"http://yoursite.com/2020/01/05/UTLK-Chapter-2-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","excerpt":"","text":"0. Address Logical Address -&gt; 段式 16bit Segment Selector 32bit Offset Linear Address (Vitual Address) -&gt; 页式 Physical Address 1. 分段一个段的描述会被分成两块： Segment Selector: 类似reference id，16bit长度，用于描述 一个Segment的index TI(Table Indicator): 描述Segment是在GDT还是LDT RPL(Requestor Privilege Level) Segment Descriptor: 描述Segment本身的信息，64bit，会有不同的类型： Code Segment Descriptor Data Segment Descriptor Task State Segment Descriptor (TSSD) Local Descriptor Table Descriptor (LDTD) 为了加速代码执行，x86本身会提供额外的寄存器来保存Segment Selector：cs,ss,ds。 2. Segment in Linux2.1. GDT每一个CPU core都会有自己的GDT，并且大部分情况下，都是一样的内容。Linux kernel使用的GDT长度为32 slots。 2.1.1. version linux 2.6.13linux 2.6.13中(UTLK中的版本)，x86的GDT定义很直接(arch/i386/kernel/head.S)： 12345678910111213141516171819202122232425262728/* * The boot_gdt_table must mirror the equivalent in setup.S and is * used only for booting. */ .align L1_CACHE_BYTESENTRY(boot_gdt_table) .fill GDT_ENTRY_BOOT_CS,8,0 .quad 0x00cf9a000000ffff /* kernel 4GB code at 0x00000000 */ .quad 0x00cf92000000ffff /* kernel 4GB data at 0x00000000 *//* * The Global Descriptor Table contains 28 quadwords, per-CPU. */ .align PAGE_SIZE_asmENTRY(cpu_gdt_table) .quad 0x0000000000000000 /* NULL descriptor */ .quad 0x0000000000000000 /* 0x0b reserved */ .quad 0x0000000000000000 /* 0x13 reserved */ .quad 0x0000000000000000 /* 0x1b reserved */ .quad 0x0000000000000000 /* 0x20 unused */ .quad 0x0000000000000000 /* 0x28 unused */ .quad 0x0000000000000000 /* 0x33 TLS entry 1 */ .quad 0x0000000000000000 /* 0x3b TLS entry 2 */ .quad 0x0000000000000000 /* 0x43 TLS entry 3 */ .quad 0x0000000000000000 /* 0x4b reserved */ .quad 0x0000000000000000 /* 0x53 reserved */ .quad 0x0000000000000000 /* 0x5b reserved */... 可以看到这里有两个gdt，一个是用于boot时候的boot_gdt_table；一个用于平时使用cpu_gdt_table。而这里的32个slot可以直接通过数得到。（笑） 2.1.2. Late Version在2.6.13之后的版本(至少在2.6.39及之后)，GDT的定义被转移到了arch/x86/kernel/cpu/common.c中： 12345678910111213DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = &#123; .gdt = &#123;#ifdef CONFIG_X86_64 [GDT_ENTRY_KERNEL32_CS] = GDT_ENTRY_INIT(0xc09b, 0, 0xfffff), [GDT_ENTRY_KERNEL_CS] = GDT_ENTRY_INIT(0xa09b, 0, 0xfffff), [GDT_ENTRY_KERNEL_DS] = GDT_ENTRY_INIT(0xc093, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER32_CS] = GDT_ENTRY_INIT(0xc0fb, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_DS] = GDT_ENTRY_INIT(0xc0f3, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_CS] = GDT_ENTRY_INIT(0xa0fb, 0, 0xfffff),#else [GDT_ENTRY_KERNEL_CS] = GDT_ENTRY_INIT(0xc09a, 0, 0xfffff), [GDT_ENTRY_KERNEL_DS] = GDT_ENTRY_INIT(0xc092, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_CS] = GDT_ENTRY_INIT(0xc0fa, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_DS] = GDT_ENTRY_INIT(0xc0f2, 0, 0xfffff), 其中的定义4.13之前： 12345#define GDT_ENTRY_INIT(flags, base, limit) &#123; &#123; &#123; \\ .a = ((limit) &amp; 0xffff) | (((base) &amp; 0xffff) &lt;&lt; 16), \\ .b = (((base) &amp; 0xff0000) &gt;&gt; 16) | (((flags) &amp; 0xf0ff) &lt;&lt; 8) | \\ ((limit) &amp; 0xf0000) | ((base) &amp; 0xff000000), \\ &#125; &#125; &#125; 4.13之后： 12345678910111213141516#define GDT_ENTRY_INIT(flags, base, limit) \\ &#123; \\ .limit0 = (u16) (limit), \\ .limit1 = ((limit) &gt;&gt; 16) &amp; 0x0F, \\ .base0 = (u16) (base), \\ .base1 = ((base) &gt;&gt; 16) &amp; 0xFF, \\ .base2 = ((base) &gt;&gt; 24) &amp; 0xFF, \\ .type = (flags &amp; 0x0f), \\ .s = (flags &gt;&gt; 4) &amp; 0x01, \\ .dpl = (flags &gt;&gt; 5) &amp; 0x03, \\ .p = (flags &gt;&gt; 7) &amp; 0x01, \\ .avl = (flags &gt;&gt; 12) &amp; 0x01, \\ .l = (flags &gt;&gt; 13) &amp; 0x01, \\ .d = (flags &gt;&gt; 14) &amp; 0x01, \\ .g = (flags &gt;&gt; 15) &amp; 0x01, \\ &#125; 其中32 slots定义如下： 12345678910struct gdt_page &#123; struct desc_struct gdt[GDT_ENTRIES];&#125; __attribute__((aligned(PAGE_SIZE)));/* * Number of entries in the GDT table: */#define GDT_ENTRIES 16#define GDT_SIZE (GDT_ENTRIES*8) 同时可以看到每一个slot的长度是64bit，8bytes。 2.2. LDTLDT在默认情况下，每一个core也只有一个，对应的descriptor分配在gdt固定的位置上。slots个数为8192，单个slot长度为8 bytes，这些定义在2.6.13后的linux版本中保持一致： 1234/* Maximum number of LDT entries supported. */#define LDT_ENTRIES 8192/* The size of each LDT entry. */#define LDT_ENTRY_SIZE 8 2.3. Kernel &amp; User Segment对于指令和数据段，Linux只是简单的使用了x86的段结构，包括： 1234567891011// --------- sgement.h/* * Segment selector values corresponding to the above entries: */#define __KERNEL_CS (GDT_ENTRY_KERNEL_CS*8)#define __KERNEL_DS (GDT_ENTRY_KERNEL_DS*8)#define __USER_DS (GDT_ENTRY_DEFAULT_USER_DS*8 + 3)#define __USER_CS (GDT_ENTRY_DEFAULT_USER_CS*8 + 3)#define __ESPFIX_SS (GDT_ENTRY_ESPFIX_SS*8) 其中后续的都是gdt的index： 123456#define GDT_ENTRY_KERNEL_CS 12#define GDT_ENTRY_KERNEL_DS 13#define GDT_ENTRY_DEFAULT_USER_CS 14#define GDT_ENTRY_DEFAULT_USER_DS 15...#define GDT_ENTRY_ESPFIX_SS 26 2. 分页64bit的x86_64的page size依然是4KB，但是包含4级页表： address size bits: 48 linear address split: 9 + 9 + 9 + 9 + 12 Linux的页表包括多层，最多4层： Page Global Directory -&gt; PGDIR_SHIFT Page Upper Directory -&gt; PUD_SHIFT = 30 Page Middle Directory -&gt; PMD_SHIFT = 21 Page Table -&gt; PAGE_SHIFT = 12 其实x86最多支持5层：5-Level Paging and 5-Level EPT。因此额外加入了一个P4D_SHIFT(Page 4th Directory)。linux因此也加入了新的patch: Intel Working On 5-Level Paging To Increase Linux Virtual/Physical Address Space 2.1. x86分页x86代码中支持2，3，4层分页，分别在不同的文件中定义： arch/x86/include/asm/pgtable-2level_types.h arch/x86/include/asm/pgtable-3level_types.h arch/x86/include/asm/pgtable_32_types.h arch/x86/include/asm/pgtable_64_types.h 12345678910111213141516171819202122232425262728293031323334353637#ifdef CONFIG_X86_5LEVEL/* * PGDIR_SHIFT determines what a top-level page table entry can map */#define PGDIR_SHIFT pgdir_shift/* * 4th level page in 5-level paging case */#define P4D_SHIFT 39#define MAX_POSSIBLE_PHYSMEM_BITS 52#else /* CONFIG_X86_5LEVEL *//* * PGDIR_SHIFT determines what a top-level page table entry can map */#define pgdir_shift 39#endif /* CONFIG_X86_5LEVEL *//* * 3rd level page */#define PUD_SHIFT 30/* * PMD_SHIFT determines the size of the area a middle-level * page table can map */#define PMD_SHIFT 21/* PAGE_SHIFT determines the page size */#define PAGE_SHIFT 12 注意这里的长度是page table的一个entry所指向的地址长度，而不是总的长度，所以这里也就看不到PTE表的SHIFT定义了。 2.2. ARMARM本身支持多种page size，linux使用了4KB，两级/三级页表： PGDIR_SHIFT -&gt; 21, 30 PMD_SHIFT -&gt; 21, 21 PAGE_SIZE -&gt; 12, 12 有意思的是2级页表的情况下，第一级页表有2048 entry(2^11)，每个entry 8bytes，包括两个指针，每一个都指向一个二级页表；不过这两个页表在内存上市连续的(每一个叫一个section)，每一个都有256个entry，合起来就有2^9 = 512 entry。 1234567#define PTRS_PER_PTE 512#define PTRS_PER_PMD 1#define PTRS_PER_PGD 2048#define SECTION_SHIFT 20#define SECTION_SIZE (1UL &lt;&lt; SECTION_SHIFT)#define SECTION_MASK (~(SECTION_SIZE-1)) ARM的三层页表是来支持huge page的。 2.3. ARM64ARM64支持最多4级页表和最大64KB page size。linux选择了： 3 (39bits)or 4 (48bits)级页表 + 4KB page size 2 (42bits)级页表 + 3 (52bits)级页表+ 64KB page size 2.4. RISC-VRISC-V本身支持非常多的virtual memory page的layout，参考： RISC-V Linux Page Table I Privileged ISA Specification linux选择了： 2级(22bits)页表 + 4KB page size 三级页表(22bits, 30bits) + 4KB page size","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"Kernel","slug":"Kernel","permalink":"http://yoursite.com/tags/Kernel/"},{"name":"Memory","slug":"Memory","permalink":"http://yoursite.com/tags/Memory/"}]},{"title":"SSA Construction and Destruction(1)","slug":"SSA-Construction-and-Destruction-1","date":"2019-12-29T07:30:22.000Z","updated":"2019-12-29T07:31:08.854Z","comments":true,"path":"2019/12/29/SSA-Construction-and-Destruction-1/","link":"","permalink":"http://yoursite.com/2019/12/29/SSA-Construction-and-Destruction-1/","excerpt":"","text":"1. SSA Construction1.1. CFG to SSA Valid TransformationTwo conditions should be satisfied: Correctness of form: Each variable mentioned in the transformed program must have exactly one definition. Semantic invariance: two coressponding paths in CFG and SSA should lead to same value for each variables at end. 1.2. Dominance Frontier dominate: n &gt;= m, if every path to m pass n strict dominate: n &gt;= m and n != m immediate dominator: n strict dominate m and n is the closet node of so -&gt; idom(m) = n Dominance Frontiers of node n are nodes m satisfy: m is not strict dominated by n exist one m pre-node p which is dominated by n we define : df(n) = { m }; Interesting things here: m may be idom(n) n may be in df(n) Consider the second case: If n is in df(n), then: there should be one cycle from n to n. It also means there’s one path from root to n which pass n intermediately and there must be one path from root n which does not contains n inside in such case, there are at least two paths reach n, leading to requirments of phi-function Normaly you may find such n in df(n) at loop header. 1.2.1. Calculate Dominace FrontierSimple but ineffcient way to calculate df is check every node m of n ( n &gt;= m), test all its successor if it is dominated by n. However, as a matter of fact, nodes each other contains relationship. Some information may reuse. We focus on dominate information. 1.2.1.1 Consititue of DF(n)First prove: If p is in df(n), then p must at least exists in one of df(m) where idom(m) = n group of successor(n) suppose p is in df(n), but neither in successor(n), nor in any of df(m) where idom(m) = n n is not strict dominate p, n !&gt; p one precessor m’ of p is dominate by n, n &gt;= m’ idom(m’) != n since p is in df(m’) in such case there must one m (idom(m) = n) dominate m’, we name one of them as m+ m+ must not dominate p, otherwise since n &gt; m+, leading to n &gt; p then we get p is in df(m+) which is conflict. However, on the other hand, not all nodes in up 2 kinds are in df(n), we need to remove those nodes x dominated by n and prove remaining nodes are really in df(n) Those nodes dominated by n should be obviously removed; For remaining nodes since it is successor node is n(n &gt;= n) or one of its successor node k &lt;= m, plusing n &gt; m, then n &gt; k. Thoese remaining nodes should be in df(n). As conclusions, we named two kinds of df(n) nodes as: df_base(n) = { m succ(n) | n !&gt; m } df_ind(n) = { p in df(m) | idom(m) = n, n !&gt; p} 1.2.1.2 Simplfy Checking ProcessWe may use idom(p) to check if n dominates the p in up process. It helps to reduce the complexity of exclusive process. We could approve, for df_base and df_ind, restriction n !&gt; m is equal to idom(m) != n. However, for df_base, n !&gt; m -&gt; idom(m) != n is obvious, since m is just successor of n. Let us focus on df_ind. Given : p is in df(m) where idom(m) = n one prec(p), named sp, m &gt;= sp m !&gt; p m could reach p n &gt;= p In such case, if idom(p) != n, let idom(p) = x, then x !&gt; m, since idom(m) = n, otherwise, idom(m) will be x. And slo m !&gt; x, otherwise m will dominate p. So, there will be two path from n to p, one pass m and one pass x, it will be conflict with x &gt; p prerequisite. Here we could get n !&gt; m -&gt; idom(m) != n is also true for base_ind. Then we could simple the df(n) as calculte: df_base(n) = { m succ(n) | idom(m) != n } df_ind(n) = { p in df(m) | idom(m) = n, idom(p) != n} idom(m) is easy to calculate in one dominate tree and we could rely on one bottomup traversal of the dominator tree to build up all df(n) for each onde: 1234567891011121314151617181920212223242526272829303132333435363738void calDF(DominateTree tree, Map&lt;DominateNode, DF&gt; output) &#123; Queue&lt;DominateNode&gt; queue; queue.push(tree.root); while (!queue.isEmpty()) &#123; DominateNode n = queue.pop(); if (!n.visited() &amp;&amp; n.hasChild()) &#123; queue.pushAll(n.children()); n.visit(); &#125; else &#123; // handle node, you can get calculated DF from output calculateDF(node, output); &#125; &#125;&#125;void calculateDF(DominateNode node, Map&lt;DominateNode, DF&gt; output) &#123; DF df; for (DominateNode sn : node.successors()) &#123; if (sn.idom() != n) &#123; df.add(sn); &#125; &#125; for (DominateNode dn : node.children()) &#123; // since visit from bottom, must be calculated DF cdf = output.get(dn); for (DominateNode cdfn : cdf) &#123; if (cdfn.idom() != n) &#123; df.add(cdfn) &#125; &#125; &#125; output.put(node, df);&#125; 1.3. Placement of phi-instructionsChoose one conservative solution: for one variable x, we place phi-instruction for it in each node of df(n) where n contains assignment of x or it has been inserted one phi-function of x.(Since phi-function itself is also one assignment of x) The whole process then is very easy, for each varialbe x: 12345678910111213141516171819Queue workList;// put all nodes contains assignment of xfor (Node n : cfg.selectNodesWithAssignment(x)) &#123; workList.push(n)&#125;while (!workList.isEmpty()) &#123; Node n = workList.pop(); // put phi-function in each node in df(n) // and put these node into workList for (Node dfn : n.df()) &#123; if (!dfn.isVisited()) &#123; dfn.insertPhi(x); workList.push(dfn) &#125; &#125;&#125; 1.4. Renaming of VariablesHere we need to take care 4 kinds of usage of one variable x: l-value(左值) in assignment r-value(右值) in assignment l-value(左值) in phi-function r-value(右值) in phi-function We will do one BFS in the tree with one stack to record context of one variable and one counter to record current version. l-value always brings one new version of variable (counter ++) and push itself into stack to update context. For r-value in assignment, just use current variable version in context; However, in phi-function we need to pick up which version shoul based on position. Another problem is when should we update the phi-function r-value: we only update phi-function r-value when it is one node’s successor. In such case, we could know path index of the node. 123456789101112131415161718192021222324252627// start with markNode(root)void markNode(Node node) &#123; // we could just save parent context here saveParentContext(); foreach(Assignment as : node.instrs()) &#123; // assign version for assignment renameWithContextAndIncCounter(as.lVariable); renameWithContext(as.rVariables()); &#125; // update phi-function in succeesor node foreach(Node sucNode: node.succs()) &#123; if (sucNode.containsPhi()) &#123; // calculate index and update related node version updatePhiRValue(node, sucNode) &#125; &#125; // call childrens foreach(Node child: node.children()) &#123; markNode(child); &#125; // pop stack back to restore envionment to parent status restoreParentContext();&#125; 1.5. Proof CorrectnessFirst define some notation: df(S) = union of all df(s) for all s in S df+(S) = limit df(df(S’)) + df(S’), until no more change join(S)={n | ∃ converging paths m -&gt; n and k -&gt; n, m,k ∈ S} join+(S) limit join(join(S’)) + join(S’), until no more change Let S be nodes containing assignments of varaible x. We need to approve: for each node in join+(S), there must be one phi-function in df+(S). And also, we need prove, after ‘placement of phi-function’ &amp; ‘Renmaing Variable’, the output is SSA and with following property: convential minimal no-pruned","categories":[],"tags":[{"name":"Complier","slug":"Complier","permalink":"http://yoursite.com/tags/Complier/"},{"name":"SSA","slug":"SSA","permalink":"http://yoursite.com/tags/SSA/"}]},{"title":"Compile LLVM & Clang @ Windows","slug":"Compile-LLVM-Clang-Windows","date":"2019-12-16T14:18:07.000Z","updated":"2019-12-16T15:06:36.202Z","comments":true,"path":"2019/12/16/Compile-LLVM-Clang-Windows/","link":"","permalink":"http://yoursite.com/2019/12/16/Compile-LLVM-Clang-Windows/","excerpt":"","text":"1. Prepare EnvironmentWe need : cmake: https://cmake.org/download/, choose windows version git: https://git-scm.com/download/win Visual Studio 2019: https://visualstudio.microsoft.com/, Community Version is enough 2. Clone LLVM projectTwo ways to get LLVM project source code git clone —config core.autocrlf=false https://github.com/llvm/llvm-project.git download source code from : http://releases.llvm.org/download.html However, the second way needs you put clang source code into llvm/tools with folder name as clang the build script will check if the folder existed and build clang project at same time if so The first way needs to add -DLLVM_ENABLE_PROJECTS flags to enable related project, the project could be clang, clang-tools-extra, libcxx, libcxxabi, libunwind, lldb, compiler-rt, lld, polly, or debuginfo-test. 3. Use CMake to generated .sln fileuse first one as example: 12345cd llvm-projectmkdir buildcd buildcmake -G &lt;generator&gt; [options] ../llvm Based on Visual Studio version you installed, the generator could be: description Visual Studio 16 2019 Generates Visual Studio 2019 project files. Use -A option to specify architecture. Visual Studio 15 2017 [arch] Generates Visual Studio 2017 project files. Optional [arch] can be “Win64” or “ARM”. Visual Studio 14 2015 [arch] Generates Visual Studio 2015 project files. Optional [arch] can be “Win64” or “ARM”. Visual Studio 12 2013 [arch] Generates Visual Studio 2013 project files. Optional [arch] can be “Win64” or “ARM”. Visual Studio 11 2012 [arch] Generates Visual Studio 2012 project files. Optional [arch] can be “Win64” or “ARM”. Visual Studio 10 2010 [arch] Generates Visual Studio 2010 project files. Optional [arch] can be “Win64” or “IA64”. Visual Studio 9 2008 [arch] Generates Visual Studio 2008 project files. Optional [arch] can be “Win64” or “IA64”. It will take some time to generate related LLVM.sln file in folder builder. 4. Open Project and CompileNow you could open the LLVM.sln, Visual Studio will automatically load all related resources to build up the LLVM project. Normally, it will take a long time (&gt;1.5h) to build up whole project in Vistual Studio. After it done its work, you could get the clang/llc binary in one debug output folder. It’s a good choice to have a meal after you start the building process and check it when you’ve enjoied the dinner. :) Have a wonderful time in LLVM world.","categories":[],"tags":[{"name":"LLVM","slug":"LLVM","permalink":"http://yoursite.com/tags/LLVM/"}]},{"title":"Compile LLVM & Clang @ Ubuntu 18.04","slug":"Compile-LLVM-Clang-Ubuntu-18-04","date":"2019-12-11T13:34:54.000Z","updated":"2019-12-11T14:07:53.819Z","comments":true,"path":"2019/12/11/Compile-LLVM-Clang-Ubuntu-18-04/","link":"","permalink":"http://yoursite.com/2019/12/11/Compile-LLVM-Clang-Ubuntu-18-04/","excerpt":"","text":"1. Prepare EnvironmentUse apt to install required pakcage: 1234sudo apt updatesudo apt upgradesudo apt install build-essential cmake python3-dev libncurses5-d make 2. Download the source codeYou may clone it from github, containting latest code and being able to upstream: 1git clone https://github.com/llvm/llvm-project.git However, it will contain all history and dev code, leading to long time when downloading the source code. If network status is poor, better to download each latest source code for llvm and clang in LLVM Download Page. Take LLVM 9.0 as example: LLVM Source Code Clang source code You will get two tar.gz: llvm-9.0.0.src.tar.xz cfe-9.0.0.src.tar.xz Clang source code should put into folder ${LLVM_SOURCE_CODE}/tools/clang(checking INSTALL.txt in clang source file). the final path as: 12345llvm-9.0.0-src ... |- tools ... |- clang (renamed from cfe-9.0.0) 3. Build Code1234mkdir buildcd buildcmake -G &quot;Unix Makefiles&quot; ../llvm-9.0.0-src It will scan CMakefiles.txt, automatically involving clang project. 4. Check Resultbin folder will be found in build folder. Setup the Path in ~/.bashrc : 1export PATH=$&#123;BUILD_FOLDER&#125;/bin:$&#123;PATH&#125; Then source ~/.bahsrc. Now checking with command clang and llc.","categories":[],"tags":[{"name":"LLVM","slug":"LLVM","permalink":"http://yoursite.com/tags/LLVM/"},{"name":"Build","slug":"Build","permalink":"http://yoursite.com/tags/Build/"}]},{"title":"野猪书第四，五，六章读书笔记","slug":"wild-pig-chapter-4-5-6","date":"2019-12-07T15:03:30.000Z","updated":"2019-12-07T15:05:04.728Z","comments":true,"path":"2019/12/07/wild-pig-chapter-4-5-6/","link":"","permalink":"http://yoursite.com/2019/12/07/wild-pig-chapter-4-5-6/","excerpt":"","text":"第四章：Overview数据结构需要改动时候，一般需要更新代码或者内部数据结构，此时会碰到两个问题： 滚动更新数据库相对复杂 依赖客户更新不可靠 这种情况下，新旧数据会同时存在系统中，需要双向兼容： 向后兼容：新代码可以读取旧数据 向前兼容：旧代码可以读取新数据 语言内置编码方案问题 和语言绑定的编码方案往往是语言specified 对象的序列化和反序列化往往需要跳过一些安全check，注入恶意代码变得有可能。比如在java默认构造函数中注入代码。 不同版本之间的实现往往是不兼容的，最明显的就是Java的jackson系列json序列化lib 性能也往往不是这些lib着重考虑的因素 数据流转方式 通过数据库 通过service call: Rest and RPC 通过异步message发送 缓冲区更好的可靠性 自动重发 message queue隐藏发送方信息 多个接收方 解耦发送和接收方，通过message queue 第五章：Overviewreplication考虑因素 single leader, multi leader and leaderless 单主节点：只有一个节点负责写入 多主节点：多节点写入 无主节点：所有节点都可以写入 数据一致性方式：synchronous或者asynchronous 可以一个节点同步，保证已经额外有一份copy，然后其他的进行异步同步，也可以叫半同步 handle failed replica read-your-writes and mono‐ tonic reads guarantees. CS模式配置新节点 一般使用snapshot的方式来配置新的节点 然后通过数据更新日志来更新新节点 处理节点失效 追赶式恢复 -&gt; 通过磁盘的日志 基于语句的复制 -&gt; 比如记录输入的SQL语句 now()，random()等函数是非确定的 自递增的column不适适用 where等语句效果取决于数据情况 带副作用的操作，比如触发器，用户定义函数 基于预写日志(WAL)传输 日志即存储，日志结构的存储引擎（SSTables和LSM-trees） 覆盖型的写入(B-tree)，日志作为内存中，尚未刷入磁盘的备份 基于行的逻辑日志复制 一系列记录数据行级，但是更加详细和准确的描述，比如mysql的binlog 基于触发器的复制 -&gt; 类似event 复制滞后的问题 读自己的写 -&gt; 也就是所谓的写后读，写完以后进行读取，此时可能读取到一个尚未得到更新数据的节点 目标就是：读写一致性 记录节点更新时间 引入最后请求时间戳 单调读 -&gt; 两次从不同的两个节点可能读到不同的数据 始终保持在一个节点上读，除非节点失效，保证不会一次读取新的以后再另外一个节点读到旧的数据。 单调读一致性 前缀一致读：一系列的数据读取得到的数据顺序和写入顺序保持一致 不相关的数据读取顺序不需要和写入顺序一致 解决方法之一：相关数据由同一节点处理 多主节点避免冲突 类似于hash，相同或者类同数据只在一个节点处理 给每一个写都带一个UUID 自定义冲突解决逻辑： 在写入时候解决，写入时候发现冲突要求解决 在读取时候解决，返回多个可能结果，让用户自己选择 自动冲突解决： 无冲突的复制数据类型 可合并的持久数据结果 操作转换 无主节点复制还是多看论文把。 第六章：数据分区分区定义：每一条数据只属于某个特定的分区。 如何分区 随机分发分区 -&gt; 代价是每次查询都需要扫描全局 基于关键字分区 首先对关键字进行排序，分段进行分区 内部可以使用SSTables和LSM-Trees保存 不合适的关键字选择容易出现热点节点 基于hash分区 不支持range搜索 -&gt; 容易退化成全局搜索 折衷 -&gt; 复合主键 一个键做hash，其他作为排序 所以核心就是选择合适的key作为hash 一致性哈希 分区和二级索引二级索引索引就是非主键索引以外的索引，而主键索引一般是作为分区来作用的。 很多数据库不支持二级索引，比如HBase。 有两种主要的方法来支持二级索引的分区 基于文档的分区 各个分区独自保存自己的二级索引 实现简单，写快，但是读时候性能难讲 更加类似于分区local的二级索引 基于词条的分区 基于全局进行索引build 同时对这个索引构建主键进行分区操作 写时候性能变差，但是读的时候优势 更新异步的话，可以减少写性能损失 分区调整： 固定数据数量 动态分区 按节点比例分区 请求路由三种方式client到数据节点： 随机选择一个节点，如果不对，那么节点进行转发 中间有一层路由层，了解所有分区信息，然后进行转发 客户端自己有所有分区信息，直接读取节点 并行查询执行多个或者复杂的查询可以拆分成更加fragement的操作，分到不同的节点，提高性能。","categories":[],"tags":[{"name":"Storage","slug":"Storage","permalink":"http://yoursite.com/tags/Storage/"},{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"系统设计，分布式","slug":"系统设计，分布式","permalink":"http://yoursite.com/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%EF%BC%8C%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"Linux Interrupt Part 1","slug":"Linux-Interrupt-Part-1","date":"2019-12-05T14:49:18.000Z","updated":"2019-12-16T15:09:22.958Z","comments":true,"path":"2019/12/05/Linux-Interrupt-Part-1/","link":"","permalink":"http://yoursite.com/2019/12/05/Linux-Interrupt-Part-1/","excerpt":"","text":"1. 中断设计constrains 中断应该是尽量能够快速响应的，否则类似网络传输的数据可能丢失 因此linux将中断处理设计成了urgent的部分+可以defer处理的部分 中断处理必须考虑重入问题，在处理一个中断时候处理其他中断 中断可以被屏蔽的，disabled 2. 中断和异常中断(interrupt) 可屏蔽中断(Maskable interrupts) 所有Device触发的Interrupt Requests(IRQs)都是可屏蔽中断 不可屏蔽中断(Nonmaskable interrupts) 一些重要的events(比如硬件故障)是不可屏蔽中断 异常(exception) Processor-detected exceptions: Generated when the CPU detects an anomalous condition when execution instruction. 进一步的细分成三种： Faults: 可恢复，返回当前运行的指令。跳转到前的指令存在EIP中 Traps: 回到下一条指令继续运行 Aborts: process只能被terminated，不过可以在这里检索错误现场 可编程的异常(Programmed exceptions): Occur at the request of the programmer. 比如INT指令或者INT 3 into或者bound指令失败时候也会触发 一般被叫做software interrupts 一般用作系统调用或者debug 所有中断和异常使用unsigned 8bits来作为index，这个值被intel称为vector。 3. IRQs and Interrupts所有的device controller都可以被指定一条或者多条Interrupt ReQest(IRQ) line. 所有existing IRQ lines都被绑定到专用硬件的input pins上。这个硬件叫Programmable Interrupt Controller(可编程中断控制器)。 PIC的具体流程如下： 检测所有连接的IRQ lines是否有raised signals。如果有多个IRQ lines被raised，那么选择lower pin的number。 如果有IRQ line被raise convert raised signal to对应的vector 将vector保存在一个Interrupt Controller I/O port，这使得cpu可以通过data bus读到这个vector send一个raised signal到CPU的INTR pin，触发一个中断 等待cpu acknowldege the interrupt signal by writting into one of the Programmable Interrupt Controllers(PIC) I/O ports. 在此之后，clear INTR line. 回到step 1 IRQ定义从0开始，也就是IRQ0，对应的intel的vector值从32开始。但是具体的对应的关系，是可以通过编程来定义的。 IRQ的line是可以选择性的ignored，但是对应的signal不会被丢失，会在un-ingore时候触发该有的操作。 注意，这里的ignore和可屏蔽的中断的屏蔽是两个概念。EFLAGS中的IF flag的屏蔽和恢复(cli and sti)都是针对于所有IRQ lines。 最早的PIC设备是2片8259A芯片，支持15个IRQs(主片的IRQ2 pin接到了从片上)。同时8259A芯片不支持多核（SMP）。 4. APIC (Advanced Programmable Interrupt Controller)为了支持SMP，引入了APIC，包括： local APIC，包含LINT0和LINT1两个pin接口，在模拟8259A芯片时候，可以一个作为INTR，一个作为NMI。 I/O APIC，作为对接设备的LB存在，负责接收device， controller的singal，然后负责分发。也能模拟8259A芯片。 可编程，提供复杂的IRQ和vector的映射关系 同时通过各自的Interrupt Command Register(ICR)，各个core之间也可以相互发送消息（中断），称为interprocessor interrupt (IPIs) I/O APIC的分发可以是static的（类似affinity），也可以使用round robin的方式。 5. Required Exceptionsvector最小的一系列exception(0-19)是intel定义，并且每个os都必须又对应的handler进行处理的。linux对于这些exception，每个注册了对应的exeption handler，这些handler大部分最终会send相对应的SIGNAL来方便其他程序监听和处理。比较有意思的几个如下： Exception Exception handler Signal 14 Page Fault page_fault( ) SIGSEGV 17 Alignment check alignment_check( ) SIGBUS 6. IDTIDT中可以保存三类gate： Task Gate：保存了一个TSS的段选择器，简单来说就是当这个exception handler被触发时候，会发生task的切换来进行处理 Interrupt Gate：保存普通的段选择器+offset，用来指向对应的处理代码入口；Interrupt Gate触发时候会clear IF flag来屏蔽中断。 Trap Gate：和Interrupt Gate一样，保存普通的段选择器+offset，但是不会屏蔽中断 总体来说，Linux uses interrupt gates to handle interrupts and trap gates to handle exceptions. 7. Interrupt Trigger ProcessTrigger process 从GDTR获取GDT的信息，然后取得对应的段描述符 比较权限，需要满足下述条件，否则throw “General Protection”: 对于非用户触发的Interrupt，CPL &lt;= DPL，也就是说interrupt handler权限要不小于触发中断的程序的权限，否则无法进行处理中断。 对于用户触发的Interrupt，需要CPL =&gt; DPL，也就是说用户的权限要不小于触发的handler的权限，否则用户可以去调用其他的interrupt或者trap gate。 如果CPL和DPL的权限不同，那就需要切换到DPL对应级别的TSS和Stack上 首先读取当前的ss和esp信息（当前stack信息） 从需要切换到对象的TSS中读取对应的ss和esp信息 将当前的ss和esp信息保存在切换后的stack中 如果是fault，那么需要同时需要保存cs和eip，等待handler后返回现场（逻辑地址）；最后保存eflags。如果有hardware error code，同样保存在stack上。 从段描述符中读取基地址，加上offset，得到目标逻辑地址并设置cs和eip，开始执行interrupt handler。 在interrupt handler完成工作后，需要返回到原有的进程中，一般通过iret命令，其包括如下的操作： 从当前stack上获取cs，eip和eflags。 如果权限之前发生过切换，那么需要恢复到原有的stack上，ss和esp同样保存在interrupt handler的stack上 恢复stack，恢复cs，eip和eflags 检查ds，es，fs和gs的DPL是否小于恢复后的CPL，如果是，那么进行清空。这是以防用户进程获取的系统权限级别的段描述符。 8. Interrupt GateGate类型： Interrupt gate DPL = 0 处理内部的中断(interrupt) System gate DPL = 3 vector 4(into), 5(bound)和128(int 0x80)使用这个gate System interrupt gate DPL = 3 int3, User Mode的Debug Trap gate DPL = 0 大部分内部异常(exception)依赖于这个 Task gate DPL = 0 handle double fault 9. IDT初始化5.4 kernel中和UTLK中的实现已经有所不同了 9.1. legacy mode在4.x版本中，初始化依然是通过”函数”： 1234567891011121314151617181920oid __init trap_init(void)&#123; int i; set_intr_gate(X86_TRAP_DE, divide_error); set_intr_gate_ist(X86_TRAP_NMI, &amp;nmi, NMI_STACK); /* int4 can be called from all */ set_system_intr_gate(X86_TRAP_OF, &amp;overflow); set_intr_gate(X86_TRAP_BR, bounds); set_intr_gate(X86_TRAP_UD, invalid_op); set_intr_gate(X86_TRAP_NM, device_not_available);#ifdef CONFIG_X86_32 set_task_gate(X86_TRAP_DF, GDT_ENTRY_DOUBLEFAULT_TSS);#else set_intr_gate_ist(X86_TRAP_DF, &amp;double_fault, DOUBLEFAULT_STACK);#endif...&#125; set_intr_gate负责设置interrupt gate： 12345678910111213141516set_intr_gate(n, addr) |- set_intr_gate_notrace(n, addr); |- _set_gate(n, GATE_INTERRUPT, (void *)addr, 0, 0, __KERNEL_CS); |- pack_gate(&amp;s, type, (unsigned long)addr, dpl, ist, seg); |- write_idt_entry(idt_table, gate, &amp;s); |- write_trace_idt_entry(gate, &amp;s); // trace |- _trace_set_gate // linux trace // write#define write_idt_entry(dt, entry, g) native_write_idt_entry(dt, entry, g)static inline void native_write_idt_entry(gate_desc *idt, int entry, const gate_desc *gate)&#123; memcpy(&amp;idt[entry], gate, sizeof(*gate));&#125; 可以看到set_intr_gate等一些列操作，最终是往内存的一个位置(idt_table)写入对应的gate信息。 idt_table在head_32.S中被定义： 123idt_descr: .word IDT_ENTRIES*8-1 # idt contains 256 entries .long idt_table 对于64bit的定义在arch/x86/kernel/cpu/common.c中： 12345#ifdef CONFIG_X86_64struct desc_ptr idt_descr __ro_after_init = &#123; .size = NR_VECTORS * 16 - 1, .address = (unsigned long) idt_table,&#125;; 随后会被lidt命令把地址写入idtr中： 12345678910111213141516/* * The load_current_idt() must be called with interrupts disabled * to avoid races. That way the IDT will always be set back to the expected * descriptor. It&apos;s also called when a CPU is being initialized, and * that doesn&apos;t need to disable interrupts, as nothing should be * bothering the CPU then. */static inline void load_current_idt(void)&#123; if (is_debug_idt_enabled()) load_debug_idt(); else if (is_trace_idt_enabled()) load_trace_idt(); else load_idt((const struct desc_ptr *)&amp;idt_descr);&#125; 而lidt的会读取desc_ptr struct的size作为limit，address作为idt的base： 123456789101112131415IF OperandSize = 16 THEN IDTR(Limit) ← SRC[0:15]; IDTR(Base) ← SRC[16:47] AND 00FFFFFFH; ELSE IF 32-bit Operand Size THEN IDTR(Limit) ← SRC[0:15]; IDTR(Base) ← SRC[16:47]; FI; ELSE IF 64-bit Operand Size (* In 64-Bit Mode *) THEN IDTR(Limit) ← SRC[0:15]; IDTR(Base) ← SRC[16:79]; FI;FI; 9.2. kernel version 5.45.4 kernel中初始化有改变，set_intr_gate没有了，或者说设置default idt的值不通过set_intr_gate。不过idt_table之类的还是在的，依旧做为idt的内存存储： 1234567891011121314151617181920212223242526/* * The default IDT entries which are set up in trap_init() before * cpu_init() is invoked. Interrupt stacks cannot be used at that point and * the traps which use them are reinitialized with IST after cpu_init() has * set up TSS. */static const __initconst struct idt_data def_idts[] = &#123; INTG(X86_TRAP_DE, divide_error), INTG(X86_TRAP_NMI, nmi), ...#ifdef CONFIG_X86_32 TSKG(X86_TRAP_DF, GDT_ENTRY_DOUBLEFAULT_TSS),#else INTG(X86_TRAP_DF, double_fault),#endif INTG(X86_TRAP_DB, debug), SYSG(X86_TRAP_OF, overflow),#if defined(CONFIG_IA32_EMULATION) SYSG(IA32_SYSCALL_VECTOR, entry_INT80_compat),#elif defined(CONFIG_X86_32) SYSG(IA32_SYSCALL_VECTOR, entry_INT80_32),#endif&#125;; 可以看到，set_intr_gate变成了INTG的宏： 1234567891011121314#define G(_vector, _addr, _ist, _type, _dpl, _segment) \\ &#123; \\ .vector = _vector, \\ .bits.ist = _ist, \\ .bits.type = _type, \\ .bits.dpl = _dpl, \\ .bits.p = 1, \\ .addr = _addr, \\ .segment = _segment, \\ &#125;/* Interrupt gate */#define INTG(_vector, _addr) \\ G(_vector, _addr, DEFAULT_STACK, GATE_INTERRUPT, DPL0, __KERNEL_CS) 这里的idt_data定义如下： 123456struct idt_data &#123; unsigned int vector; unsigned int segment; struct idt_bits bits; const void *addr;&#125;; 而def_idts的值会在函数idt_setup_from_table中写入： 123456789101112static voididt_setup_from_table(gate_desc *idt, const struct idt_data *t, int size, bool sys)&#123; gate_desc desc; for (; size &gt; 0; t++, size--) &#123; idt_init_desc(&amp;desc, t); write_idt_entry(idt, t-&gt;vector, &amp;desc); if (sys) set_bit(t-&gt;vector, system_vectors); &#125;&#125; 该这么多是为了保证代码整洁，原来的32和64bit定义分开，这里统一到了一起。 9.3. ignore irq handler在irq handler初始化之前，所有的对应的idt entry都会被设置成 head_32.S中的early_ignore_irq(老版本是ignore_int)，代码基本一致： 1234567891011121314151617181920212223242526272829303132/* This is the default interrupt &quot;handler&quot; :-) */ENTRY(early_ignore_irq) cld#ifdef CONFIG_PRINTK pushl %eax pushl %ecx pushl %edx pushl %es pushl %ds movl $(__KERNEL_DS),%eax movl %eax,%ds movl %eax,%es cmpl $2,early_recursion_flag je hlt_loop incl early_recursion_flag pushl 16(%esp) pushl 24(%esp) pushl 32(%esp) pushl 40(%esp) pushl $int_msg call printk call dump_stack addl $(5*4),%esp popl %ds popl %es popl %edx popl %ecx popl %eax#endif iret 9.4 Typical Exception Handler Process基本的exception处理流程包括如下几步： 保存error code，jmp到通用的exception handler流程 legacy mode: error_code latest(5.4): common_exception 通用的exception handler流程会首先保存context 调用对应的c代码的handler(地址保存在%edi中) 调用ret_from_exception开始返回 调用restore_all_kernel 检查是否有中断 恢复stack frame随后iret 更加完整的流程描述如下： Saves the registers that might be used by the high-level C function on the stack. Issues a cld instruction to clear the direction flag DF of eflags, thus making surethat autoincreases on the edi and esi registers will be used with stringinstructions.* Copies the hardware error code saved in the stack at location esp+36 in edx.Stores the value –1 in the same stack location. As we’ll see in the section “Reexecution of System Calls” in Chapter 11, this value is used to separate 0x80 exceptions from other exceptions. Loads edi with the address of the high-level do_handler_name( ) C functionsaved in the stack at location esp+32; writes the contents of es in that stacklocation. Loads in the eax register the current top location of the Kernel Mode stack. Thisaddress identifies the memory cell containing the last register value saved instep 1. Loads the user data Segment Selector into the ds and es registers. Invokes the high-level C function whose address is now stored in edi. 以下，举处除零的handler为例。 9.4.1. Legacy do_divide_error实现12345678ENTRY(divide_error) RING0_INT_FRAME ASM_CLAC pushl_cfi $0 # no error code pushl_cfi $do_divide_error jmp error_code CFI_ENDPROCEND(divide_error) error_code是共享的处理流程，其中会调用到do_divide_error函数： 1234567891011error_code: ... # push registers cld ... # build up do_divide_error function stack call *%edi # call do_divide_error function jmp ret_from_exception CFI_ENDPROCEND(page_fault) do_divide_error是个c程序，定义在traps.c中的宏DO_ERROR_INFO，最终会调用do_trap来： 1234567891011121314151617181920212223#define DO_ERROR_INFO(trapnr, signr, str, name, sicode, siaddr) \\dotraplinkage void do_##name(struct pt_regs *regs, long error_code) \\&#123; \\ ... conditional_sti(regs); \\ do_trap(trapnr, signr, str, regs, error_code, &amp;info); \\ exception_exit(prev_state); \\&#125;// do_trapstatic void __kprobesdo_trap(int trapnr, int signr, char *str, struct pt_regs *regs, long error_code, siginfo_t *info)&#123; struct task_struct *tsk = current; ... if (info) force_sig_info(signr, info, tsk); else force_sig(signr, tsk);&#125; 最后一步是想当前程序发送一个SIGFPE signal。ret_from_exception和5.4版本中功能类似，后面一起讲。 9.4.2. Linux 5.4 kernel实现：9.4.2.1. 32bit divide_error代码在entry_32.S中 123456ENTRY(divide_error) ASM_CLAC pushl $0 # 这里保存error code，0代表没有 pushl $do_divide_error # 这里保存目标c程序地址 jmp common_exceptionEND(divide_error) common_exception是通用的exception处理流程（除了double fault）： 123456789101112131415161718192021common_exception: /* the function address is in %gs&apos;s slot on the stack */ SAVE_ALL switch_stacks=1 skip_gs=1 ENCODE_FRAME_POINTER UNWIND_ESPFIX_STACK /* fixup %gs */ GS_TO_REG %ecx movl PT_GS(%esp), %edi # get the function address REG_TO_PTGS %ecx SET_KERNEL_GS %ecx /* fixup orig %eax */ movl PT_ORIG_EAX(%esp), %edx # get the error code movl $-1, PT_ORIG_EAX(%esp) # no syscall to restart TRACE_IRQS_OFF movl %esp, %eax # pt_regs pointer CALL_NOSPEC %edi # 跳转到对应的c历程 jmp ret_from_exceptionEND(common_exception) 除去一堆保存环境和测试的，这里要关注的就是跳转到c handler的代码。 9.4.2.2. 64bit divide_error实现代码在entry_64.S中： 123idtentry divide_error do_divide_error has_error_code=0// idtentry - Generate an IDT entry stub idtentry: generates an IDT stub that sets up a usable kernel context, creates struct pt_regs, and calls @do_sym. 9.4.2.2 c handlerdo_divide_error实现在arch\\x86\\kernel\\traps.c 123456789101112131415161718192021222324252627#define IP ((void __user *)uprobe_get_trap_addr(regs))#define DO_ERROR(trapnr, signr, sicode, addr, str, name) \\dotraplinkage void do_##name(struct pt_regs *regs, long error_code) \\&#123; \\ do_error_trap(regs, error_code, str, trapnr, signr, sicode, addr); \\&#125;DO_ERROR(X86_TRAP_DE, SIGFPE, FPE_INTDIV, IP, \"divide error\", divide_error)static void do_error_trap(struct pt_regs *regs, long error_code, char *str, unsigned long trapnr, int signr, int sicode, void __user *addr)&#123; RCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\"); /* * WARN*()s end up here; fix them up before we call the * notifier chain. */ if (!user_mode(regs) &amp;&amp; fixup_bug(regs, trapnr)) return; if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) != NOTIFY_STOP) &#123; cond_local_irq_enable(regs); do_trap(trapnr, signr, str, regs, error_code, sicode, addr); &#125;&#125; 类似的，这里也会调用的通用do_trap实现中去。 9.5. 返回完成调用以后，ret_from_exception负责返回到原来进程中。 123456ret_from_exception - restore_all_kernel - .Lno_preempt - .Lirq_return - IRET_FRAME -&gt; 回复堆栈 - INTERRUPT_RETURN -&gt; iret 或者 jmp native_iret 10. 中断(Interrupt) 异常最终的操作基本都是发signal给当前的进程，这是因为异常的发生往往是in time的。 但是对于中断，中断的trigger往往是异步的，触发时候目标进程可能在sleep状态，也就是说当前进程不是中断的目标进程。 中断细节可以分成如下几类： I/O interrupts Timer interrupts local APIC timer or an external timer Interprocessor interrupts 10.1. I/O interrupts多台设备可能绑在同一个I/O interrupt上，使用同一条IRQ line。此时，需要一种分享的方式，一般有两种方式： IRQ sharing: 多个设备使用同一个interrupt service routines (ISRs)，这个routine需要去判断到底哪一个设备发送了中断。 IRQ dynamic allocation: 一次只让一个设备使用，只有激活状态的设备可以尝试独占IRQ line。 另一方面，对于中断处理程序，因为其会屏蔽中断，并且可能被switch出去，所以linux会把中断处理后可能的操作分成三类： Critical Noncritical Noncritical deferrable","categories":[],"tags":[{"name":"Micro","slug":"Micro","permalink":"http://yoursite.com/tags/Micro/"},{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"Interrupt","slug":"Interrupt","permalink":"http://yoursite.com/tags/Interrupt/"}]},{"title":"野猪书第三章读书笔记","slug":"wild-pig-chapter3","date":"2019-11-21T06:10:59.000Z","updated":"2019-12-05T15:04:13.542Z","comments":true,"path":"2019/11/21/wild-pig-chapter3/","link":"","permalink":"http://yoursite.com/2019/11/21/wild-pig-chapter3/","excerpt":"","text":"1. overview数据库分类 事务处理型 数据分析型 存储索引家族： 日志结构的存储引擎 面向页的存储引擎，比如b-tree 2. 哈希表索引核心：将key hash到对应的文件位置 使用日志方式，不修改已有记录，在末尾增加 如果能够保持key在内存中，那么写入速度会很快 将日志分段，到一定大小，生成新的段 在合适时机进行段合并，只保留最新的结果 使用后台线程，在完成合并之前使用老的index 合并之后切换到新的index，删除老的index 最终，每一个段都会有自己的索引（注意，可能有多个段） 搜索需要从最新的段开始往前找索引表 在实现中还需要考虑： 文件格式：二进制 &gt; CSV 删除记录：可以通过标记，在合并时候处理被删除的key 崩溃回复：如果重启引起内存中的key map丢失，可以 从头开始扫描文件，重建key map 或者在平时就定时将key map映射到磁盘之上 部分写入的记录：写日志中也有可能崩溃，加入校验值来发现损坏情况。 并发控制：因为段是需要严格顺序写入的，因此 写必须是单线程 读可以多线程 为什么之追加不直接修改文件： 顺序写性能会好很多，特别对于HDD。但是对于SSD或者nvme磁盘如何呢？ 因为追加是只读的，因此兵法和崩溃恢复都简单很多。 合并旧段可以减轻文件碎片化问题 哈希表索引局限性： 索引哈希表必须全部放入内存，如果spill到磁盘上，性能会收到影响；另外，哈希变满时候，哈希冲突会影响性能。 区间查询效率不高：就是说相邻的key的value值分布基本不会是相邻的，因此扫描一个range的keys，需要逐个查找其中的每一个key。 换句话说，你没办法知道range中哪些key是存在的，只能一个一个测试。 3. SSTable索引哈希表索引中保存在文件中的key-value值对是不排序的，出现的顺序基本是按照写入的顺序的来的，无论是原始文件还是合并后的文件。 如果我们要求磁盘上的文件值对，都是按照key的顺序来保存。那么会有如下好处： 合并两个segment file会变得高效 内存中的索引不需要保存所有key的索引信息，只需要保存几个作为标点的key的索引，其他key可通过区间扫描来寻找。在区间不大时候，需要扫描的range是有限的，性能方面也是非常快的。 文件可以进行压缩，我们只需要记录segment的start和end的key 如何实现： 在内存中引入排序结构，比如红黑树或者AVL-tree，在抵达threshold之后写入磁盘 查找的时候按照先内存后磁盘的方法进行搜寻 定时后台合并文件 为了支持崩溃恢复，可以为内存中的数据额外做一份普通的日志来作为恢复使用 3.1. LSM-Tree所有类似这种排序后的segment file合并的结构都可以称之LSM-Tree(Log-structured merge-tree)。 包括LevelDB,RocksDB和Cassandra，Hbase都有LSM-tree的影子。另外ES以来的Lucence索引引擎，也用了类似的逻辑：倒排索引按照key的顺序保存在文件中，通过额外的索引的索引来快速搜索这些倒排索引。 注意SSTable是索引，LSM-Tree是真正存储数据的文件结构。 3.2. 优化最坏情况下，如果某个key不存在或者key只存在最老的segment中时候，需要触发一次全segment文件的扫描操作。对于key不存在的情况，引入blooming filter，保证如果filter告诉你不存在，保证key确实不存在。 对于后者，SSTable的压缩和合并逻辑会有一些影响，有两种合并逻辑： 大吞小 分层合并：将旧的数据分到单独的层级，然后进行合并 4. B-TreeB-Tree将数据和索引分成固定大小的数据块，一般为4KB（配合x86的默认页大小4KB），然后将数据块们组织成树形结构。注意这里，B-Tree保存的是索引信息，每一个叶子节点保存了某个值的具体位置信息。 参考： B-Tree MySQL索引背后的数据结构及算法原理 简单来说，一个M维度的B-Tree节点可以如此描述： 123456struct Node &#123; int numberKeys; string keys[M - 1]; object values[M - 1]; struct Node children[M];&#125; 另外有一些约束来保证B-Tree的搜索性能，对于一棵order为m的树（最多有m个子节点）： 任何节点如果有k个key，那么就有k+1个ref 除了叶子节点，每一层至少是半满的，也就是至少有[m/2]个key和key+1的ref；最多有有m-1个key和m个ref 每个internal 节点至少有2个子节点（2个ref） 在这种情况下，整棵树的高度可以限制在(N为key的个数）： H = \\frac{log_{[m/2]}(N+1)}{2}注意一棵b树的每个节点既保存key对应的值，又保存了ref的信息。 4.1 B-Tree容错B-Tree一般使用预写日志(write-ahead, WAL)，先写日志，再写Tree。 多线程情况下， B-Tree不像LSM-Tree那么容易。 4.2 B-Tree优化 写时复制 B+ Tree，也就是在internal节点不保存具体节点的值，只保存ref或者其他索引信息；在叶子节点才保存具体的值。这样做的另外一个好处是可以把非叶子节点都加载到内存中。 slibing节点直接可以增加额外的ref来快速遍历 分形树（FTI，Fractal Tree Indexes） 为叶子节点增加存储数据的buffer，组织成一棵树 为节点增加Message buffer，用于缓冲读写请求 5. LSM-Tree和B-Tree比较写放大：合并数据时候引入的额外写入。 LSM-Tree优势： 有时候具有较低的写放大 顺序写人更加好的cache效率 更好的压缩和更少的碎片 LSM-Tree缺点： 后台合并操作影响前台读写性能，相对而言，B-Tree的性能更加稳定 高吞吐量时候，如果后台合并速度跟不上，磁盘空间最终会被吃完。 B-Tree更好的支持锁机制。在许多关系数据库中，事务隔离是通过key范围上的锁来实现的。 6.其他6.1. 二级索引除了主索引外的额外索引，但是会有key重复的问题。一般通过拼凑其他信息来使得key唯一。 6.2. 堆文件一般索引中不保存具体的值，只是保存一个ref信息或者位置信息，具体的内容保存在堆文件中。 在更新时候，如果新值不大于旧值，那么直接覆盖就可以了；否则需要重新分配空间，并且所有指向这个值的ref信息都需要更新或者留下一个指向新位置的间接跳转信息。 6.3. 聚集索引索引中直接保存具体的值。另外有保存部分值的覆盖索引或者包含列索引。（类似cache） 无论那种都可以提高读取性能，代价是复杂的插入和事务同步问题。 6.4. 多列索引对于多列进行合并索引。 级联索引 把多个列合并拼接起来作为一个key索引。对于A+B拼接的，可以对于A+B或者A进行快速索引，但是没有办法对B单独进行快速索引。 多维索引 对于多个列进行真正的索引。传统的B-Tree和LSM-Tree都没有办法高效的应对这种查询。 6.4.1. R-TreeR树是用来做空间数据存储的树状数据结构。例如给地理位置，矩形和多边形这类多维数据创建索引。 类似于B-Tree，R-Tree将空间划分成多个子空间（类似于一个range），然后继续划分直到最终值，效果如下： 与此相关的还有其他的名为空间索引的机制： 参考文章 https://cloud.tencent.com/developer/news/199266 https://zhuanlan.zhihu.com/p/38597148 空间索引列表： GeoHash kd-tree Grid index 四叉树/八叉树 Space filling curve：通过一条线来描述空间上的所有位置，二维值可以转化成单个值 LSH（Locality Sensitive Hashing 6.5. 全文搜索全文搜索不同于之前的索引有明显的范围。全文搜索往往是模糊的。 lucene通过给key构建前缀树来提供key的模糊搜索。 7. 内存数据库 随机访问 -&gt; 可以使用磁盘没有办法高效实用的数据结构 易失性 -&gt; NVM 标准产品： redis RAMCloud 8. 事务处理（OLTP，online transaction processing)OLTP类似于传统的SQL引擎，查询没有固定的format，更多是ad-hoc(online)的查询。OLAP（online analytic processing），类似于数据仓库。OLTP的索引更多使用上述提到的索引；OLAP需要新设计的索引。 Property Transaction processing systems (OLTP) Analytic systems (OLAP) Main read pattern |Small number of records per query, fetched by key |Aggregate over large number of recordsMain write pattern |Random-access, low-latency writes from user input |Bulk import (ETL) or event streamPrimarily used by |End user/customer, via web application |Internal analyst, for decision supportWhat data represents |Latest state of data (current point in time) |History of events that happened over timeDataset size |Gigabytes to terabytes |Terabytes to petabytes 典型的数据仓库 Terdata Vertica SAP HAHA Hadoop系列 9. 列式存储 按照列的方式来存储数据，而不是传统的整行的顺序 只需要关注几列 -&gt; 更少的数据读取和更快的读取速度 方便压缩 位图进一步的游程(Run-Length)编码 cache &amp; memory friend，并且可以使用SIMD指令 列式存储的排序和索引 HBase列簇中的数据式按照行来存储的。 10. Data Cubes and Materialized Views(物化视图)物化会提前计算出一些结果然后保存下来。Data Cube就是按照各种不同维度聚合形成的数据块。 优势是预先计算能获得好的性能提升。 劣势缺乏灵活性和额外的存储空间。","categories":[],"tags":[{"name":"Storage","slug":"Storage","permalink":"http://yoursite.com/tags/Storage/"},{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"系统设计","slug":"系统设计","permalink":"http://yoursite.com/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"name":"索引","slug":"索引","permalink":"http://yoursite.com/tags/%E7%B4%A2%E5%BC%95/"}]},{"title":"RSIC-V RV32I Instruction Set","slug":"RSIC-V-RV32I-Instruction-Set","date":"2019-11-18T14:28:50.000Z","updated":"2019-12-16T15:09:45.447Z","comments":true,"path":"2019/11/18/RSIC-V-RV32I-Instruction-Set/","link":"","permalink":"http://yoursite.com/2019/11/18/RSIC-V-RV32I-Instruction-Set/","excerpt":"","text":"1. Instruction LengthRISC-V指令是定长的32bit，但是拓展可以支持变长。变长一定是16bit的整数倍。 encoding convension要求所有32bit指令长度的命令最低两位一定是11. 注意RISC-V是小端的指令集。对于&gt;32bit长度的指令格式，同样要求满足这个要求，因此对于16bit指令长度，要求最低两位一定不是11。 当然对于non-standard指令格式，RISC-V可以是大端的。 2. RV32IRV32I是最基础的指令集，也是任何RISC-V实现必须实现的指令集。RV32I包括了47条独特指令，另外，实现可以选择使用总是trap的系统（SYSTEM）硬件指令代替 8 条 SCALL / SBREAK / RD* 指令，可以讲指令集减少到40条；如果还能够实现FENCE和FENC.I，那么可以将指令总数减少到38条。RV32I 能够模拟几乎所有的 ISA 扩展（除了 A 扩展，它需要额外的硬件来支持原子性（atomicity））。 RV32I was designed to be sufficient to form a compiler target and to support modern operating system environments. The ISA was also designed to reduce the hardware required in a minimal implementation. RV32I contains 47 unique instructions, though a simple implementation might cover the eight SCALL/SBREAK/CSRR* instructions with a single SYSTEM hardware instruction that always traps and might be able to implement the FENCE and FENCE.I instructions as NOPs, reducing hardware instruction count to 38 total. RV32I can emulate almost any other ISA extension (except the A extension, which requires additional hardware support for atomicity). RV32I包括了32个32bit的通用寄存器(x0-x31)外加一个用户可见的pc寄存器。32个通用寄存器其中x0固定为全0，x1一般用于保存返回值。详细的register convension参考较早文章。 2.1. opcode layoutRISC-V设计几个理念： 所有的layout格式尽量共享位置的意义的定义 比如所有的指令的r0，r1，rd位置都固定index 尽量减少指令额外的计算，比如预先进行shift，只使用符号扩展 RV32I包括了4种基本格式(R/I/S/U)和两种变化格式(B/J)： 可以看到寄存器的位置是固定的。 2.2. 整数指令首先需要注意到，opcode的长度是8个bit，去掉固定为11的最低两位，能用的opcode长度是6bit，也就是说最多支持2^6=64条指令。但实际上，RV32I中的很多指令是公用opcode的，通过额外参数来区分具体的action。 比如ADD和SUB就共享opcode。完整的列表如下： LUI: load imm into register AUIPC: add imm to pc JAL: unconditional direct jump JALR: unconditional indirect jump BEQ: conditional branch when equal BNE: conditional branch when not equal BLT: conditional branch when r1 &lt; r2 BGE: conditional branch when r1 &gt; r2 BLTU: conditional branch when r1 &lt; r2 as unsinged BGEU: conditional branch when r1 &gt; r2 as unsinged LB: load 8bit value from memory into register LH: load 16bit value from memory into register LW: load 32bit value from memory into register LBU: load 8bit unsigned value from memory into register LHU: load 16bit unsigned value from memory into register SB: store 8bit value from register to memory SH: store 16bit value from register to memory SW: store 32bit value from register to memory ADDI: adds the sign-extended 12-bit immediate with r1 to rd SLTI: put 1 into register if r1 &lt; signed extend imm otherwise 0 SLTIU: put 1 into register if r1 &lt; r2 extend imm as unsigned otherwise 0 XORI: xor the sign-extended 12-bit immediate with r1 to rd ORI: or the sign-extended 12-bit immediate with r1 to rd ANDI: and the sign-extended 12-bit immediate with r1 to rd SLLI: logical left shift register value imm bits SRLI: logical right shift register value imm bits SRAI: arithmetic right shift register value imm bitsshift ADD: add SUB: sub SLL: logical left shift r1 value low 5 bits value of r2 SLT: put 1 into register if r1 &lt; r2 otherwise 0 SLTU: put 1 into register if r1 &lt; r2 as unsignedotherwise 0 XOR: xor SRL: logical right shift r1 value low 5 bits value of r2 SRA: arithmetic right shift r1 value low 5 bits value of r2 OR: or AND: and FENCE: memory fence ECALL：Trap to System Call EBREAK: DEBUG mode break 一些注解 RISC-V的FENCE可以玩组合: Any combination of device input (I), device output (O), memory reads (R), and memory writes (W) may be ordered with respect to any combination of the same. RISC-V是没有Overflow Flag: 下面引自官方文档： We did not include special instruction-set support for overflow checks on integer arithmetic operations in the base instruction set, as many overflow checks can be cheaply implemented using RISC-V branches. Overflow checking for unsigned addition requires only a single additional branch instruction after the addition: add t0, t1, t2; bltu t0, t1, overflow. For signed addition, if one operand’s sign is known, overflow checking requires only a single branch after the addition: addi t0, t1, +imm; blt t0, t1, overflow. This covers the common case of addition with an immediate operand. For general signed addition, three additional instructions after the addition are required, leveraging the observation that the sum should be less than one of the operands if and only if the other operand is negative. 1234add t0, t1, t2slti t3, t2, 0slt t4, t0, t1bne t3, t4, overflow In RV64I, checks of 32-bit signed additions can be optimized further by comparing the results of ADD and ADDW on the operands. 简单来说，检查是否overflow没有flag给你用（为了简化电路设计，另外overflow用处不大），而是用指令+branch来进行判断，详细而言分成3中情况： unsigned相加，那么没有overflow意味着 rd &gt; r1 &amp;&amp; rd &gt; r2，当然对于imm版本只需要检测rd &gt; r1 signed imm相加，如果imm符号知道，那么判断rd &gt; r1或者rd &lt; r1 剩下的就需要好几条指令来判断了 NOP命令: 不存在的，可以用ADD r1, r1, r0来代替。拓展指令集可能加入单独的NOP指令。 状态寄存器: 不存在的+1。老版本的文档将CSR相关命令列到RV32I下面；当前文档(draft-20191114-0777770)单独拆到了一章：“Zicsr”, Control and Status Register(CSR) Instructions, Version 2.0 因此也不存在用于比较的EFLAGS，所有的条件跳转都是比较具体的两个寄存器或者寄存器和imm的值。 说到底也是为了简化系统的设计。 3. RV64I和RV64I非常类似，差别在于： 寄存器宽度变成了64bit 扩展了64bit宽度的指令LD/SD (double word) 增加了单独处理32bit的指令(ADD之类的现在等是64bit的) ADDIW/SLLW/SRLW/SUBW/SRAW 注意移位操作offset bits还是5bit 移位操作的offset bits因为64bit=32bit*2，从5bit”升级”成6bit 其实RV128I指令集的拓展也类似，理论上可以拓展到imm无法支持offset bits的描述，但在此之前，量子计算机应该出来吧。 Reference https://content.riscv.org/wp-content/uploads/2017/05/riscv-spec-v2.2.pdf risc-v manual: https://github.com/riscv/riscv-isa-manual http://gfiles.chinaaet.com/scorpio/group/20170425/4000264810-6362872848946855461428672.pdf","categories":[],"tags":[{"name":"Micro,RISC-V,ISA","slug":"Micro-RISC-V-ISA","permalink":"http://yoursite.com/tags/Micro-RISC-V-ISA/"}]},{"title":"野猪书第二章读书笔记","slug":"wild-pig-chapter2","date":"2019-11-18T08:33:11.000Z","updated":"2019-11-18T08:34:25.400Z","comments":true,"path":"2019/11/18/wild-pig-chapter2/","link":"","permalink":"http://yoursite.com/2019/11/18/wild-pig-chapter2/","excerpt":"","text":"1. 模型 层次化模型(hierachy) 树状结构 -&gt; keep one-to-many or many-to-one no many-to-many 关系化模型 -&gt; sql 范式 在复杂的或者多层的one-to-many结构中需要拆成多张表 文档化模型(document) -&gt; json 一列中可以保存复杂结构的数据 比如列表形式的历史记录 对于不复杂的one-to-many特别适合 但是对于many-to-many不是那么有优势 join不是那么友好，许多数据库都不支持join 网络化模型(network) 图状结构,many-to-many 搜索和优化变得困难 图计算数据库 2. 存储模型规格化的（normalized): 所有重复的数值(主要是string)会被应设成id 统一的格式和语法 避免歧义 i18n的时候分离了显示和存储要求，更加方便迁移 更快的search [坏处]规格化的数据需要多张表，search时候需要更加多的join 非规格化的(denormalized): 如果数据库本身不支持join 可能对于cache hit更加友好 3. schema设计 读时schema模式（所谓无模式）：在读的时候检查需要的shchema是否合理 写时schema模式，在写的时候做检查，否则不让写入 实际上会是混合模式，写时会做一部分，读的时候做一部分。 这些设计在column发生修改时候变的有所不同。 读时schema可能只需要写入新的field的值就可以了： 123if (user &amp;&amp; user.name &amp;&amp; !user.first_name) &#123; user.first_name = user.name.split(&quot; &quot;)[0]&#125; 写时schema需要更新数据库table的schema 123ALTER TABLE users ADD COLUMN first_name text;UPDATE users SET first_name = split_part(name, &quot; &quot;, 1); -- PostgreSQLUPDATE users SET first_name = substring_index(name, &quot; &quot;, 1); -- MySQL schema的更新(ALTER)是件代价不小的操作，另外UPDATE操作对于大表和有复杂关联的表系列也有不小的代价。一般而言可以将新加入的field设置为NULL，在读的时候进行设置。 所以在这种情况下，如果数据中包含了不一致的schema结构（数据异构），例如： 有许多的数据格式 数据系统的schema结构不稳定，会发生变动 4. 查询的局部性 分成多张表的数据存储格式在读取连续数据时候会有一定的性能影响，需要多次索引 修改数据时候，最好的是原地覆盖，因此建议写入的文档尽量小，并且不增加数据大小 所以，为了提高局部性，可以 父表内嵌入子表数据（类似json） 多索引集群表 列式存储，列簇 5. 数据查询语言 声明式：比如SQL，只是给出了我想要什么，给出范围，但是没有给出具体的操作流程 命令式：定义为完整的操作流程，执行的命令。 声明式隔离了底层的实现逻辑，使得： API变的简洁和容易理解 提供优化的机会 提供并行执行的实现方式 6. 图状数据库 基本组成”顶点” + “边”。 相关算法：导航，PageRank 图状数据库模型： 属性图（property graph） Neon4j, Titan和InfiniteGraph 三元存储模型（triple-store） Datomic, AllegroGraph 查询语句 声明式查询语句：Cypher, SPARQL, Datalog 命令式查询语句：Gremlin 图处理框架: Pregel 6.1. 属性图在属性图模型中，一个顶点包含了： UUID Input/Output edge list property set (key-value pair) 一个边包含了： UUID edage start/end node label describe relationship between start node and end node property set (key-value pair) 在完成这些定义后，属性图可以很方便的使用多张关系数据库表来进行存储 12345678910111213CREATE TABLE vertices ( vertex_id integer PRIMARY KEY, properties json);CREATE TABLE edges ( edge_id integer PRIMARY KEY, tail_vertex integer REFERENCES vertices (vertex_id), head_vertex integer REFERENCES vertices (vertex_id), label text, properties json);CREATE INDEX edges_tails ON edges (tail_vertex);CREATE INDEX edges_heads ON edges (head_vertex); 属性图提供了非常强大的灵活性来描述一张图的结构。 6.2. Cypher基本思路是定义一个节点或者一条边需要满足的条件，查询的具体细节不关注，比如，我们想要查询这一样一个节点（人）： person，通过节点label为BORN_IN的outgoing edge，以及系列WITH_IN outgoing edage，最终抵达某个标注为US的节点 person，通过节点label为LIVES_IN的outgoing edge，以及系列WITH_IN outgoing edage，抵达某个标注为US的节点 最后打印出来person的名字： 1234MATCH (person) -[:BORN_IN]-&gt; () -[:WITHIN*0..]-&gt; (us:Location &#123;name:&apos;United States&apos;&#125;), (person) -[:LIVES_IN]-&gt; () -[:WITHIN*0..]-&gt; (eu:Location &#123;name:&apos;Europe&apos;&#125;)RETURN person.name 如果用SQL来做，会成多次的表的join。 123456789101112131415161718192021222324252627282930313233343536373839WITH RECURSIVE -- in_usa is the set of vertex IDs of all locations within the United States in_usa(vertex_id) AS ( SELECT vertex_id FROM vertices WHERE properties-&gt;&gt;'name' = 'United States' UNION SELECT edges.tail_vertex FROM edges JOIN in_usa ON edges.head_vertex = in_usa.vertex_id WHERE edges.label = 'within' ), -- in_europe is the set of vertex IDs of all locations within Europe in_europe(vertex_id) AS ( SELECT vertex_id FROM vertices WHERE properties-&gt;&gt;'name' = 'Europe' UNION SELECT edges.tail_vertex FROM edges JOIN in_europe ON edges.head_vertex = in_europe.vertex_id WHERE edges.label = 'within' ), -- born_in_usa is the set of vertex IDs of all people born in the US born_in_usa(vertex_id) AS ( SELECT edges.tail_vertex FROM edges JOIN in_usa ON edges.head_vertex = in_usa.vertex_id WHERE edges.label = 'born_in' ), -- lives_in_europe is the set of vertex IDs of all people living in Europe lives_in_europe(vertex_id) AS ( SELECT edges.tail_vertex FROM edges JOIN in_europe ON edges.head_vertex = in_europe.vertex_id WHERE edges.label = 'lives_in' ) SELECT vertices.properties-&gt;&gt;'name'FROM vertices-- join to find those people who were both born in the US *and* live in EuropeJOIN born_in_usa ON vertices.vertex_id = born_in_usa.vertex_idJOIN lives_in_europe ON vertices.vertex_id = lives_in_europe.vertex_id; 6.3. 三元存储模型（triple-store）三元存储模型和属性图类似，但是颗粒度更加细。所有的描述都是一个三维的tuple： (subject, predicate, object). subject一定是某个节点 predicate是一个edge或者一个属性 object可以是一个节点（edge）或者一个属性值（属性） 比如，可以这样来描述一张图： 123456789101112131415@prefix : &lt;urn:example:&gt;._:lucy a :Person._:lucy :name &quot;Lucy&quot;._:lucy :bornIn _:idaho._:idaho a :Location._:idaho :name &quot;Idaho&quot;._:idaho :type &quot;state&quot;._:idaho :within _:usa._:usa a :Location._:usa :name &quot;United States&quot;._:usa :type &quot;country&quot;._:usa :within _:namerica._:namerica a :Location._:namerica :name &quot;North America&quot;._:namerica :type &quot;continent&quot;. 可以压缩一下： 12345@prefix : &lt;urn:example:&gt;._:lucy a :Person; :name &quot;Lucy&quot;; :bornIn _:idaho._:idaho a :Location; :name &quot;Idaho&quot;; :type &quot;state&quot;; :within _:usa._:usa a :Location; :name &quot;United States&quot;; :type &quot;country&quot;; :within _:namerica._:namerica a :Location; :name &quot;North America&quot;; :type &quot;continent&quot;. 7. 其他模型 针对DNA（只有四个值）的基因组数据，GenBank 针对海量数据数据库 针对全文搜索的数据库","categories":[],"tags":[]},{"title":"RISC-V 寄存器 overview","slug":"RISC-V-寄存器-overview","date":"2019-11-16T16:11:47.000Z","updated":"2019-11-17T02:24:52.238Z","comments":true,"path":"2019/11/17/RISC-V-寄存器-overview/","link":"","permalink":"http://yoursite.com/2019/11/17/RISC-V-%E5%AF%84%E5%AD%98%E5%99%A8-overview/","excerpt":"","text":"1. OverviewRISC-V顾名思义是一个RISC指令集，有一些特性（特别和x86相比）： 大量的寄存器 固定为0的寄存器(x0): “零寄存器”（zero register，注：其值永远为零） 非load和save指令不能访问内存，只能在寄存器间操作 不提供context save和restore的命令 所有指令都是固定长度的32bit 指令长度固定32bit使得处理64bit的立即数，一定至少要两条指令。 1.1. 指令集和拓展RISC-V指令集最基本的指令只包含integer instruction，包括RV32I和RV64I。其中RV32I是社区声明过已经冻结修改的。（其实RV64I算是拓展，而且也被冻结了） 具体的指令情况如下（来自维基百科）： 指令集名称 描述 版本 状态 基本指令集 RV32I 基本整数指令集, 32位 2.0 冻结 RV32E 基本整数指令集(嵌入式系统), 32位, 16 寄存器 1.9 开放 RV64I 基本整数指令集, 64位 2.0 冻结 RV128I 基本整数指令集, 128位 1.7 开放 标准扩展指令集 M|整数乘除法标准扩展|2.0|冻结A|不可中断指令(Atomic)标准扩展|2.0|冻结F|单精确度浮点运算标准扩展|2.0|冻结D|双倍精确度浮点运算标准扩展|2.0|冻结G|所有以上的扩展指令集以及基本指令集的总和的简称|不适用|不适用Q|四倍精确度浮点运算标准扩展|2.0|冻结L|十进制浮点运算标准扩展|0.0|开放C|压缩指令标准扩展|2.0|冻结B|位操作标准扩展|0.36|开放J|动态指令翻译标准扩展|0.0|开放T|顺序存储器访问标准扩展|0.0|开放P|单指令多数据流（SIMD）运算标准扩展|0.1|开放V|向量运算标准扩展|0.2|开放N|用户中断标准扩展|1.1|开放 2. Register基础的寄存器包括32个整数寄存器（嵌入式版本是16个整数寄存器），也就是RV32I的寄存器，x0-x31。其中x0是固定为0的寄存器，而因为寄存器够用，所以没有单独的Stack寄存器。 引入RV32F和RV32D会引入32个浮点寄存器，f0-f31。 RISC-V的ABI中定义了各个寄存器的calling convension： Register ABI Name Description Saver x0| zero |Hard-wired zero |—x1 |ra |Return address |Callerx2 |sp |Stack pointer |Calleex3 |gp |Global pointer |—x4 |tp |Thread pointer |—x5–7 |t0–2 |Temporaries |Callerx8 |s0/fp |Saved register/frame pointer |Calleex9 |s1 |Saved register |Calleex10–11 |a0–1 |Function arguments/return values |Callerx12–17 |a2–7 |Function arguments |Callerx18–27 |s2–11 |Saved registers |Calleex28–31 |t3–6 |Temporaries |Callerf0–7 |ft0–7 |FP temporaries |Callerf8–9 |fs0–1 |FP saved registers |Calleef10–11 |fa0–1 |FP arguments/return values |Callerf12–17 |fa2–7 |FP arguments |Callerf18–27 |fs2–11 |FP saved registers |Calleef28–31 |ft8–11 |FP temporaries |Caller Reference参考： RISC-V 手册 维基百科: RISC-V 关于RISC-V你所需要知道的一切 Github","categories":[],"tags":[{"name":"Micro","slug":"Micro","permalink":"http://yoursite.com/tags/Micro/"},{"name":"RISC-V","slug":"RISC-V","permalink":"http://yoursite.com/tags/RISC-V/"}]},{"title":"ARM ISA Overview","slug":"ARM-Register-Quick-View","date":"2019-11-14T09:23:35.000Z","updated":"2019-11-27T11:53:41.530Z","comments":true,"path":"2019/11/14/ARM-Register-Quick-View/","link":"","permalink":"http://yoursite.com/2019/11/14/ARM-Register-Quick-View/","excerpt":"","text":"1. 指令ARM中能看到的指令集包括： thumb: 16bit的指令集 thumb-2: thumb的升级版，16bit/32bit混合指令集 arm: 32bit的指令集 A32: ARMv8中的arm指令集 T32: ARMv8中的thumb-2指令集，同时包含一些拓展 A64: ARMv8的64bit的指令集 Neon: ARM的SIMD指令集 ARMv8支持两种state： AArch64: 使用A64指令集，一种32bit fix length的指令集 支持31个64bit的通用寄存器 使用64bit的PC和SP寄存器 支持32个128bit的SIMD寄存器 AArch32: 支持A32或者T32指令集 支持32个32bit的通用寄存器 使用32bit的PC和SP寄存器 支持32个64bit的SIMD寄存器 注意的是ARMv8的A32和T32向前兼容的同时，还包括了其他一些拓展。 2. ARM ProfileARM系列一般都能看到A，R和M系列，分别代表着：普通应用，实时计算和移动处理的三种不同的需求。 具体以ARMv8为例子，下面摘自ARM的官方文档： A Application profile: Supports a Virtual Memory System Architecture (VMSA) based on a Memory Management Unit (MMU). Supports the A64, A32, and T32 instruction sets. R Real-time profile: Supports a Protected Memory System Architecture (PMSA) based on a Memory Protection Unit (MPU).-Supports the A32 and T32 instruction sets. M Microcontroller profile, described in this manual: Implements a programmers’ model designed for low-latency interrupt processing, with hardware stacking of registers and support for writing interrupt handlers in high-level languages. Optionally implements a variant of the R-profile PMSA. Supports a variant of the T32 instruction set. 3. LLVM ARMv7 Register我们可以在lib/target/ARM/ARMRegisterInfo.td中找到所有寄存器的定义： 1234567891011121314// Registers are identified with 4-bit ID numbers.class ARMReg&lt;bits&lt;16&gt; Enc, string n, list&lt;Register&gt; subregs = [], list&lt;string&gt; altNames = []&gt; : Register&lt;n, altNames&gt; &#123; let HWEncoding = Enc; let Namespace = &quot;ARM&quot;; let SubRegs = subregs; // All bits of ARM registers with sub-registers are covered by sub-registers. let CoveredBySubRegs = 1;&#125;class ARMFReg&lt;bits&lt;16&gt; Enc, string n&gt; : Register&lt;n&gt; &#123; let HWEncoding = Enc; let Namespace = &quot;ARM&quot;;&#125; 上述是两个基类，定义了整数寄存器和浮点寄存器的基类。总结来说，ARMv7的数据存储寄存器包括： r1-r12 32bit integer register pc, lr, sp 32bit integer register q0-q16 128bit integer register sub-reg: d0-d15 64bit float register sub-reg: s0-s31 32bit float register sub-reg: d16-d31 64bit float regiter 一般而言，有些寄存器有固定/约定的用处： pc == Program Counter lr == Link Register sp == Stack Pointer r12 == ip (scratch) r7 == Frame Pointer (thumb-style backtraces) r9 == May be reserved as Thread Register r11 == Frame Pointer (arm-style backtraces) r10 == Stack Limit 另外还有一系列控制寄存器，以下仅举例后续用到再做分析。除了APSR，大部分其他控制寄存器都是要在非用户模式下才能操作： Program Status Register APSR: Application Program Status Register, 32bit integer register 类似与x86的EFLAGs，用于记录condition flag。 CPSR: Current Program Status Register, 32bit integer register 似乎CPSR是APSR的alias。在用户模式叫APSR，在其他模式叫CPSR。 SPSR: Saved Program Status Register, 32bit integer register 仅在非用户模式下使用，用于保存CPSR的信息，方便跳转回来时候回复环境。 FPSCR: Floating-Point Status and Control Register, 32bit integer register 浮点状态寄存器。 Float Special Registe(Only privileged mode) FPSID: Floating-Point System ID Register, 32bit integer register 浮点系统设置寄存器，用来表明NEON/VFP等实现是否被使用。 FPEXC: Floating-point Exception Register, 32bit integer register 用于表明在exception时候如何处理，特别是在NEON/VFP模式下面。 FPINST and FPINST2Floating-Point Instruction Registers, 32bit integer register ITSTATE: Thumb if-then指令状态寄存器。 3.1. 通用寄存器定义 r0 - r15 12345678910111213141516171819202122// Integer registersdef R0 : ARMReg&lt; 0, &quot;r0&quot;&gt;, DwarfRegNum&lt;[0]&gt;;def R1 : ARMReg&lt; 1, &quot;r1&quot;&gt;, DwarfRegNum&lt;[1]&gt;;def R2 : ARMReg&lt; 2, &quot;r2&quot;&gt;, DwarfRegNum&lt;[2]&gt;;def R3 : ARMReg&lt; 3, &quot;r3&quot;&gt;, DwarfRegNum&lt;[3]&gt;;def R4 : ARMReg&lt; 4, &quot;r4&quot;&gt;, DwarfRegNum&lt;[4]&gt;;def R5 : ARMReg&lt; 5, &quot;r5&quot;&gt;, DwarfRegNum&lt;[5]&gt;;def R6 : ARMReg&lt; 6, &quot;r6&quot;&gt;, DwarfRegNum&lt;[6]&gt;;def R7 : ARMReg&lt; 7, &quot;r7&quot;&gt;, DwarfRegNum&lt;[7]&gt;;// These require 32-bit instructions.let CostPerUse = 1 in &#123; // extra cost when usingdef R8 : ARMReg&lt; 8, &quot;r8&quot;&gt;, DwarfRegNum&lt;[8]&gt;;def R9 : ARMReg&lt; 9, &quot;r9&quot;&gt;, DwarfRegNum&lt;[9]&gt;;def R10 : ARMReg&lt;10, &quot;r10&quot;&gt;, DwarfRegNum&lt;[10]&gt;;def R11 : ARMReg&lt;11, &quot;r11&quot;&gt;, DwarfRegNum&lt;[11]&gt;;def R12 : ARMReg&lt;12, &quot;r12&quot;&gt;, DwarfRegNum&lt;[12]&gt;;let RegAltNameIndices = [RegNamesRaw] in &#123; // 定义别名def SP : ARMReg&lt;13, &quot;sp&quot;, [], [&quot;r13&quot;]&gt;, DwarfRegNum&lt;[13]&gt;;def LR : ARMReg&lt;14, &quot;lr&quot;, [], [&quot;r14&quot;]&gt;, DwarfRegNum&lt;[14]&gt;;def PC : ARMReg&lt;15, &quot;pc&quot;, [], [&quot;r15&quot;]&gt;, DwarfRegNum&lt;[15]&gt;;&#125;&#125; q0 - q15 q系列的寄存器包含了一系列的子寄存器，因此需要一系列的SubRegisterIndex来定义他们的关系： 12def dsub_0 : SubRegIndex&lt;64&gt;;def dsub_1 : SubRegIndex&lt;64, 64&gt;; 第一条描述了qx中的d(2x)index：从0开始，长度为64bit；第二条描述了d(2x+1)index：从64bit开始，长度64bit。 123456789101112131415161718192021// Advanced SIMD (NEON) defines 16 quad-word aliaseslet SubRegIndices = [dsub_0, dsub_1] in &#123;def Q0 : ARMReg&lt; 0, &quot;q0&quot;, [D0, D1]&gt;;def Q1 : ARMReg&lt; 1, &quot;q1&quot;, [D2, D3]&gt;;def Q2 : ARMReg&lt; 2, &quot;q2&quot;, [D4, D5]&gt;;def Q3 : ARMReg&lt; 3, &quot;q3&quot;, [D6, D7]&gt;;def Q4 : ARMReg&lt; 4, &quot;q4&quot;, [D8, D9]&gt;;def Q5 : ARMReg&lt; 5, &quot;q5&quot;, [D10, D11]&gt;;def Q6 : ARMReg&lt; 6, &quot;q6&quot;, [D12, D13]&gt;;def Q7 : ARMReg&lt; 7, &quot;q7&quot;, [D14, D15]&gt;;&#125;let SubRegIndices = [dsub_0, dsub_1] in &#123;def Q8 : ARMReg&lt; 8, &quot;q8&quot;, [D16, D17]&gt;;def Q9 : ARMReg&lt; 9, &quot;q9&quot;, [D18, D19]&gt;;def Q10 : ARMReg&lt;10, &quot;q10&quot;, [D20, D21]&gt;;def Q11 : ARMReg&lt;11, &quot;q11&quot;, [D22, D23]&gt;;def Q12 : ARMReg&lt;12, &quot;q12&quot;, [D24, D25]&gt;;def Q13 : ARMReg&lt;13, &quot;q13&quot;, [D26, D27]&gt;;def Q14 : ARMReg&lt;14, &quot;q14&quot;, [D28, D29]&gt;;def Q15 : ARMReg&lt;15, &quot;q15&quot;, [D30, D31]&gt;;&#125; 可以看到这里的let SubRegIndices = [dsub_0, dsub_1]。 3.2. 控制寄存器的定义相对没有什么花头，简单的定义，略掉一些。 12345678910111213141516// Current Program Status Register.// We model fpscr with two registers: FPSCR models the control bits and will be// reserved. FPSCR_NZCV models the flag bits and will be unreserved. APSR_NZCV// models the APSR when it&apos;s accessed by some special instructions. In such cases// it has the same encoding as PC.def CPSR : ARMReg&lt;0, &quot;cpsr&quot;&gt;;def APSR : ARMReg&lt;15, &quot;apsr&quot;&gt;;def APSR_NZCV : ARMReg&lt;15, &quot;apsr_nzcv&quot;&gt;;def SPSR : ARMReg&lt;2, &quot;spsr&quot;&gt;;def FPSCR : ARMReg&lt;3, &quot;fpscr&quot;&gt;;def FPSCR_NZCV : ARMReg&lt;3, &quot;fpscr_nzcv&quot;&gt; &#123; let Aliases = [FPSCR];&#125;def ITSTATE : ARMReg&lt;4, &quot;itstate&quot;&gt;;... 唯一可以注意的是，为了一些操作方便，这里对于NZCV的flags做了alias的定义。 3.3. 定义RegisterClass随后需要告诉LLVM如何使用这些寄存器，或者说哪些寄存器是一类的。比如，对于通用寄存器的定义： 1234567891011121314151617181920212223242526def GPR : RegisterClass&lt;&quot;ARM&quot;, [i32], 32, (add (sequence &quot;R%u&quot;, 0, 12), SP, LR, PC)&gt; &#123; // Allocate LR as the first CSR since it is always saved anyway. // For Thumb1 mode, we don&apos;t want to allocate hi regs at all, as we don&apos;t // know how to spill them. If we make our prologue/epilogue code smarter at // some point, we can go back to using the above allocation orders for the // Thumb1 instructions that know how to use hi regs. let AltOrders = [(add LR, GPR), (trunc GPR, 8), (add (trunc GPR, 8), R12, LR, (shl GPR, 8))]; let AltOrderSelect = [&#123; return MF.getSubtarget&lt;ARMSubtarget&gt;().getGPRAllocationOrder(MF); &#125;]; let DiagnosticString = &quot;operand must be a register in range [r0, r15]&quot;;&#125;// GPRs without the PC. Some ARM instructions do not allow the PC in// certain operand slots, particularly as the destination. Primarily// useful for disassembly.def GPRnopc : RegisterClass&lt;&quot;ARM&quot;, [i32], 32, (sub GPR, PC)&gt; &#123; let AltOrders = [(add LR, GPRnopc), (trunc GPRnopc, 8), (add (trunc GPRnopc, 8), R12, LR, (shl GPRnopc, 8))]; let AltOrderSelect = [&#123; return MF.getSubtarget&lt;ARMSubtarget&gt;().getGPRAllocationOrder(MF); &#125;]; let DiagnosticString = &quot;operand must be a register in range [r0, r14]&quot;;&#125; 这里定义了： class包含了哪些寄存器 选择的顺序 diagnostic信息 4. LLVM ARMv8 RegisterARMv8的通用寄存器升级到了64bit，包含了 X0-X31, 最后三个为FR,LR,SR,XZR sub-register：所有X寄存器的低32bit定义为：W0-W31，最后两个为WSP,WZR V0-V31, 类似于ARMv7中的Q0-Q31, 增加了一倍的浮点寄存器 d,s和ARMv7一致，额外增加了8bit的b类型寄存器 v是用来引用新增的vector model的。 EFLAGS被单独移出去了，更加详细控制寄存器信息，等到对应的命令时候再描述。 4.1. 通用寄存器定义12345678910111213141516171819202122232425262728def W0 : AArch64Reg&lt;0, &quot;w0&quot; &gt;, DwarfRegNum&lt;[0]&gt;;def W1 : AArch64Reg&lt;1, &quot;w1&quot; &gt;, DwarfRegNum&lt;[1]&gt;;def W2 : AArch64Reg&lt;2, &quot;w2&quot; &gt;, DwarfRegNum&lt;[2]&gt;;def W3 : AArch64Reg&lt;3, &quot;w3&quot; &gt;, DwarfRegNum&lt;[3]&gt;;def W4 : AArch64Reg&lt;4, &quot;w4&quot; &gt;, DwarfRegNum&lt;[4]&gt;;def W5 : AArch64Reg&lt;5, &quot;w5&quot; &gt;, DwarfRegNum&lt;[5]&gt;;...def W28 : AArch64Reg&lt;28, &quot;w28&quot;&gt;, DwarfRegNum&lt;[28]&gt;;def W29 : AArch64Reg&lt;29, &quot;w29&quot;&gt;, DwarfRegNum&lt;[29]&gt;;def W30 : AArch64Reg&lt;30, &quot;w30&quot;&gt;, DwarfRegNum&lt;[30]&gt;;def WSP : AArch64Reg&lt;31, &quot;wsp&quot;&gt;, DwarfRegNum&lt;[31]&gt;;def WZR : AArch64Reg&lt;31, &quot;wzr&quot;&gt;, DwarfRegAlias&lt;WSP&gt;;let SubRegIndices = [sub_32] in &#123;def X0 : AArch64Reg&lt;0, &quot;x0&quot;, [W0]&gt;, DwarfRegAlias&lt;W0&gt;;def X1 : AArch64Reg&lt;1, &quot;x1&quot;, [W1]&gt;, DwarfRegAlias&lt;W1&gt;;def X2 : AArch64Reg&lt;2, &quot;x2&quot;, [W2]&gt;, DwarfRegAlias&lt;W2&gt;;def X3 : AArch64Reg&lt;3, &quot;x3&quot;, [W3]&gt;, DwarfRegAlias&lt;W3&gt;;def X4 : AArch64Reg&lt;4, &quot;x4&quot;, [W4]&gt;, DwarfRegAlias&lt;W4&gt;;def X5 : AArch64Reg&lt;5, &quot;x5&quot;, [W5]&gt;, DwarfRegAlias&lt;W5&gt;;...def X24 : AArch64Reg&lt;24, &quot;x24&quot;, [W24]&gt;, DwarfRegAlias&lt;W24&gt;;def X28 : AArch64Reg&lt;28, &quot;x28&quot;, [W28]&gt;, DwarfRegAlias&lt;W28&gt;;def FP : AArch64Reg&lt;29, &quot;x29&quot;, [W29]&gt;, DwarfRegAlias&lt;W29&gt;;def LR : AArch64Reg&lt;30, &quot;x30&quot;, [W30]&gt;, DwarfRegAlias&lt;W30&gt;;def SP : AArch64Reg&lt;31, &quot;sp&quot;, [WSP]&gt;, DwarfRegAlias&lt;WSP&gt;;def XZR : AArch64Reg&lt;31, &quot;xzr&quot;, [WZR]&gt;, DwarfRegAlias&lt;WSP&gt;;&#125; 没有什么特别的。 4.2. SIMD &amp; FP寄存器普通的定义没有什么特别，只是加了一个b寄存器。 123456789101112131415161718192021222324252627def B0 : AArch64Reg&lt;0, &quot;b0&quot;&gt;, DwarfRegNum&lt;[64]&gt;;...def B31 : AArch64Reg&lt;31, &quot;b31&quot;&gt;, DwarfRegNum&lt;[95]&gt;;let SubRegIndices = [bsub] in &#123;def H0 : AArch64Reg&lt;0, &quot;h0&quot;, [B0]&gt;, DwarfRegAlias&lt;B0&gt;;...def H31 : AArch64Reg&lt;31, &quot;h31&quot;, [B31]&gt;, DwarfRegAlias&lt;B31&gt;;&#125;let SubRegIndices = [hsub] in &#123;def S0 : AArch64Reg&lt;0, &quot;s0&quot;, [H0]&gt;, DwarfRegAlias&lt;B0&gt;;...def S31 : AArch64Reg&lt;31, &quot;s31&quot;, [H31]&gt;, DwarfRegAlias&lt;B31&gt;;&#125;let SubRegIndices = [ssub], RegAltNameIndices = [vreg, vlist1] in &#123;def D0 : AArch64Reg&lt;0, &quot;d0&quot;, [S0], [&quot;v0&quot;, &quot;&quot;]&gt;, DwarfRegAlias&lt;B0&gt;;...def D31 : AArch64Reg&lt;31, &quot;d31&quot;, [S31], [&quot;v31&quot;, &quot;&quot;]&gt;, DwarfRegAlias&lt;B31&gt;;&#125;let SubRegIndices = [dsub], RegAltNameIndices = [vreg, vlist1] in &#123;def Q0 : AArch64Reg&lt;0, &quot;q0&quot;, [D0], [&quot;v0&quot;, &quot;&quot;]&gt;, DwarfRegAlias&lt;B0&gt;;...def Q31 : AArch64Reg&lt;31, &quot;q31&quot;, [D31], [&quot;v31&quot;, &quot;&quot;]&gt;, DwarfRegAlias&lt;B31&gt;;&#125; SubRegisterIndex定义不再赘述。注意这里sub-register再一个parent-register没有两个了，因为通用寄存器本身翻倍了。 4.3. Register Vector定义有意思的是LLVM是如何定义ARMv8中的vector value的使用的。 首先需要看到的是LLVM对于value type的定义，在LLVM/include/Target/ValueTypes.td中： 12345678910111213...def v8i16 : ValueType&lt;128, 38&gt;; // 8 x i16 vector valuedef v16i16 : ValueType&lt;256, 39&gt;; // 16 x i16 vector valuedef v32i16 : ValueType&lt;512, 40&gt;; // 32 x i16 vector valuedef v64i16 : ValueType&lt;1024,41&gt;; // 64 x i16 vector valuedef v128i16: ValueType&lt;2048,42&gt;; //128 x i16 vector valuedef v1i32 : ValueType&lt;32 , 43&gt;; // 1 x i32 vector valuedef v2i32 : ValueType&lt;64 , 44&gt;; // 2 x i32 vector valuedef v3i32 : ValueType&lt;96 , 45&gt;; // 3 x i32 vector valuedef v4i32 : ValueType&lt;128, 46&gt;; // 4 x i32 vector valuedef v5i32 : ValueType&lt;160, 47&gt;; // 5 x i32 vector value... 这些ValueType会被用在定义RegisterClass时候： 123456789101112131415def FPR64 : RegisterClass&lt;&quot;AArch64&quot;, [f64, i64, v2f32, v1f64, v8i8, v4i16, v2i32, v1i64, v4f16], 64, (sequence &quot;D%u&quot;, 0, 31)&gt;;// We don&apos;t (yet) have an f128 legal type, so don&apos;t use that here. We// normalize 128-bit vectors to v2f64 for arg passing and such, so use// that here.def FPR128 : RegisterClass&lt;&quot;AArch64&quot;, [v16i8, v8i16, v4i32, v2i64, v4f32, v2f64, f128, v8f16], 128, (sequence &quot;Q%u&quot;, 0, 31)&gt;;// The lower 16 vector registers. Some instructions can only take registers// in this range.def FPR128_lo : RegisterClass&lt;&quot;AArch64&quot;, [v16i8, v8i16, v4i32, v2i64, v4f32, v2f64, v8f16], 128, (trunc FPR128, 16)&gt;;","categories":[],"tags":[{"name":"LLVM","slug":"LLVM","permalink":"http://yoursite.com/tags/LLVM/"},{"name":"ARM","slug":"ARM","permalink":"http://yoursite.com/tags/ARM/"},{"name":"Micro","slug":"Micro","permalink":"http://yoursite.com/tags/Micro/"}]},{"title":"野猪书第一章读书笔记","slug":"wild-pig-chapter1","date":"2019-11-12T15:02:48.000Z","updated":"2019-12-16T15:09:50.479Z","comments":true,"path":"2019/11/12/wild-pig-chapter1/","link":"","permalink":"http://yoursite.com/2019/11/12/wild-pig-chapter1/","excerpt":"","text":"数据密集型应用系统 数据库 高速缓存 索引 流式处理 批处理 这里比较有意思的是索引，一般而言，索引不会单独领出来说，往往集成在数据库中。但实际上索引也许可以单独作为一个组件存在。而这些组件往往也不是独立存在，比如Es就包含了前三部分。 另外，现在的数据体系中，不会是单独一个组件而存在，而是多个组件或者说上述所有组件的共同合作结果。 比如一个响应用户请求的系统： 需要一个高速缓存来快速响应某些通用请求 需要一个数据库来存储所有的信息 需要一个全文索引来方便查找 需要一个消息队列来处理异步任务 三个系统设计关键问题 可靠性 提供客户需要的功能 容忍客户错误或者不正确的操作 足够强的性能 足够强的安全性 可扩展性 可维护性 可靠性首先需要注意的是，可靠性并不意味着我们需要对所有可能发生的故障或者意外事件都要进行处理。追求无穷的可靠性，意味着无穷的成本。 因此，可靠性或者说容错是有针对性的，或者说有范围的。 MTTF：平均无故障事件 错误类型： 硬件错误：一般而言可以通过硬件冗余进行处理 软件错误：相对难有快速解决方案，只能通过经验或者监控来缓解 人为失误：比如配置错误 精心设计API 沙盒或者dry run 充分测试 亡羊补牢，这需要快速回滚等支持 监控 可扩展性首先的问题是如何描述性能，一般而言关心吞吐量(throughput)，也就是从server角度来看，单位时间内能够处理请求的数目。吞吐量和latency是相关的，throughput = 1 / latency。 另外一个角度是从客户角度来看，也就是响应时间（response time），从客户发送请求，到返回结果的间隔。 这些描述，一般而言可以从统计学的角度来看，也就是平均数，中位数， 95%（比95%的请求都要高），99%甚至99.99%。这些统计信息往往和服务质量目标（Service Level Objective）或者服务质量协议(Service Level Agreements)绑定在一起。 其次，对于扩展，可以从两个方面看： 保持资源不变，增加负载会如何 保持性能不变，增加多少资源才能满足增加负载的需求 应付扩展 垂直扩容：升级机器性能 水平扩容：增加更加多的机器（可以利用廉价机器） 有状态服务一般会优先考虑垂直扩容，直到实在无法满足需求才考虑水平扩容；而分布式或者无服务的服务，水平扩容十分合适。但实际中，还是service speicified。 例如，即使两个系统的数据吞吐量折算下来是一样的，但是为每秒处理1000,000次请求(每个大小为KB)而设计的系统，与每分钟3个请求（每个大小2GB ）设计的系统会大不相同。 而且系统需求本身也是难以在一开始就被定义好的，整个service生命周期可能会发生变动或者其他情况。因此，快速迭代是一种增加扩展型的办法。 可维护性维护他人留下的系统往往是令人不快的。换个角度，在设计之时就可以特别关注设计的三个原则： 可运维性：方便运营团队来保持系统稳定 监控 自动化 排除单点热点 良好的文档和易于理解的操作模式 良好的默认设置 尝试自我修复 log，方便回顾 简单性：简化系统设计，方便工程师理解系统 使用通用”语言”，比如SQL，而不是新发明 定义好好的抽象，通用语言其实也是一种抽象 可演化性：后续工程师能轻松进行改进来满足需求变更，或者说能够快速演化，这需要好的抽象等等","categories":[],"tags":[{"name":"Storage","slug":"Storage","permalink":"http://yoursite.com/tags/Storage/"},{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"系统设计","slug":"系统设计","permalink":"http://yoursite.com/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"}]}]}